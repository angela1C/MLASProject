{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics Project 2020\n",
    "by Angela Carpenter\n",
    "- [power production dataset](https://raw.githubusercontent.com/ianmcloughlin/2020A-machstat-project/master/dataset/powerproduction.csv) from Ian McLoughlin Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- Project Instructions / Overview\n",
    "- Project plan\n",
    "- About this Notebook\n",
    "- Python Libraries\n",
    "- Downloading and running the code\n",
    "- Loading / Reading in the dataset\n",
    "- Exploring the dataset\n",
    "- Describing the dataset\n",
    "- Summary Statistics\n",
    "- Visualising the dataset using plots\n",
    "- Summary and Conclusions\n",
    "- References\n",
    "\n",
    "\n",
    "## Project Instructions\n",
    "\n",
    "In this project you must create a web service that uses machine learning to make pre- dictions based on the data set powerproduction available on Moodle. The goal is to produce a model that accurately predicts wind turbine power output from wind speed values, as in the data set. You must then develop a web service that will respond with predicted power values based on speed values sent as HTTP requests. Your submission must be in the form of a git repository containing, at a minimum, the following items.\n",
    "1. Jupyter notebook that trains a model using the data set. In the notebook you should explain your model and give an analysis of its accuracy.\n",
    "2. Python script that runs a web service based on the model, as above.\n",
    "3. Dockerfile to build and run the web service in a container.\n",
    "4. Standard items in a git repository such as a README.\n",
    "To enhance your submission, you might consider developing and comparing more than one model. Rest assured, all the above concepts will be explored in lecture videos and other materials in the coming semester.\n",
    "\n",
    "### notes to self\n",
    "\n",
    "The goal of this project is to produce a model that accurately predicts wind turbine power output from wind speed values, as in the data set. The first time I looked at this I was wondering if the data values represented measurements over a particular period of time and if the lower values of power might be related to the time taken for the turbines to get up and running before they could generate any power or if some of the zero power values were related to periodic maintenance when the turbines might get switched off. There does seem to be some linear relationship between the two variables for the middle portion of the dataset and the power generated does seem to level off after a particular point but overall the relationship is strong but non-linear.\n",
    "\n",
    "However looking at the raw data in the csv file, the rows are ordered by ascending values of speed column with the corresponding power values in the other column. There is no indication as to how the data values were collected and therefore I don't think I can make any judgements about the data over a particular timeframe. There are only two columns of numerical data provided with no indication provided as to their measurement units.  I will just focus on trying to use some algorithms to predict the power values based on the input values of speed.  \n",
    "\n",
    "- whether to keep all zeros in or out\n",
    "- scaling the input and output values\n",
    "- I trained a neural network with and without the zeros, definitely the learning curve looks much better when the zeros are dropped. I first only dropped the zero values for the very high speed values as the turbines would be switched off in dangerous winds. I then tried dropping all, then some!\n",
    "- I also need to look at the web app side of it so I won't get carried away on training the model until I have this up and running even if the model is not perfect. It seems to be a case of playing around with the parameters and it is surprising how well the NN models the data given only single numerical values fed into it.\n",
    "- dfx is a copy of df with some zeros dropped (speed values over 24.4)\n",
    "- dfz is a copy of df with all zeros dropped, or maybe it was zeros where power was over a certain value. Anyway its near the end of the notebook.\n",
    "- I will probably lose all the numpy polyfit regression models and maybe leave in the higher order polynomial using scikit learn.\n",
    "- I might look again at scikit learn to see if there are any other models to try. if I have time that it...\n",
    "- I do have docker installed.\n",
    "- I am following some machine learning mastery tutorials which are really informative. Also the keras for engineers.\n",
    "- cleaned up some unnecesary statistics below and consolidated the exploratory plots.\n",
    "- Need to tidy up the section on a brief background of wind turbines, just to see how it relates to the s-curve and not going into the physics of how wind turbines actually work. \n",
    "- I did the seabron regression plots on the full dataset so I might just try it quickly on the cleaned dataset without all the zero values.\n",
    "- Need to decide on final parameters for the keras neural network and save a model to use with a flask app.\n",
    "- Need to set up the flask app and virtual envirnoment. First clean up this notebook some more.\n",
    "- the h5 file does not seem to be correct. maybe there is another format to save it as.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first importing the following libraries\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save link to data and reference the link \n",
    "csv_url = 'https://raw.githubusercontent.com/ianmcloughlin/2020A-machstat-project/master/dataset/powerproduction.csv'\n",
    "# read the csv data into a pandas dataframe\n",
    "df = pd.read_csv(csv_url)\n",
    "df\n",
    "# write the dataframe to csv\n",
    "df.to_csv('df.csv')\n",
    "# make a copy of the dataframe\n",
    "dfx = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power variable represents wind turbine power output and the speed values are wind speed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [DataFrame.to_numpy()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy) function `df.to_numpy()` gives a NumPy representation of the underlying data. It converts the dataframe to a numpy array which may be useful here given that the data is all numerical types (floats64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first few rows in the dataset: \\n\\n\", df.head(3))  # look at the top 5 rows of the DataFrame df\n",
    "print('\\n The final few rows in the dataset \\n',df.tail(3)) # Look at the bottom 5 rows of the DataFrame\n",
    "print(\"The index of the DataFrame: \", df.index) # the index or row labels of the DataFrame\n",
    "print(\"The dtypes in the dataframe are:\", end='\\n\\n')\n",
    "\n",
    "print(df.dtypes) # the data types attributes for each column in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero and Null values\n",
    "\n",
    "There are no null values in the dataset but there are some zero values. Null values are not zero in Python.\n",
    "While there is only one zero value for the `speed` variable, there are 49 zero values for the `power` variable.\n",
    "\n",
    "(Will look at the distribution of values to see if the zero values are very unusual compared to the remaining 451 values for the power variable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "df.notnull().sum()\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50767452/check-if-dataframe-has-a-zero-element\n",
    "0 in df.values\n",
    "df.isin([0]).any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29229600/counting-number-of-zeros-per-row-by-pandas-dataframe\n",
    "\n",
    "df.isin([0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='speed').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='speed', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='power', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='power').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='speed').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis generally involves both non-graphical methods which include calculation of summary statistics and graphical methods which summarises the data in a picture or a plot. These methods can be univariate where one variable is looked at at a time or multivariate where two or more variables are looked at together to explore relationships. First I will do so some univariate EDA on both of the components before going on to do multivariate EDA. \n",
    "Plots can highlight any obvious relationships between the different variables in the dataset. They can also be used to identify any groups of observations that are clearly separate to other groups of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distribution of speed and power values\n",
    "Histograms are used to show the distribution of a single quantitative variable such as speed or power values, including the centre and spread of the distribution and if there is any skew in the data. \n",
    "\n",
    "The histogram of wind speed looks to be uniformly distributed with wind speed values spread out from zero up to 25 which seems to be the max speed value. Power values on the other hand looks to be bimodal with two defined peaks, one around zero power values and the second around the 100 kilowatt mark. \n",
    "\n",
    "The wind turbine power values seem to have a peak at very low values. This is not surprising given the large number of zero power values in this dataset. Almost 10% of the power values supplied are zero.  There is another peak around values of 95-100. Most of the remaining power values fall between 18 and 85.\n",
    "A scatter plot can be used to see if there is any visible relationship between the two variables. \n",
    "\n",
    "The scatter plot shows a non-linear relationship between wind speed values and power. There does appear to be a linear relationship for mid-range speed values only.\n",
    "- When wind speed increases from very low levels, the power does not increase by much or at all. \n",
    "- The wind turbine power values does start to increase with the wind speed from wind speed values of about 5 to about 18/19. \n",
    "- From wind speeds of 20 onwards to about 25 the power values increase no further and seem to level off. \n",
    "\n",
    "From looking at the curves and knowing very little about wind, you would guess that at very low winds there may not be enough speed to get the turbine up and running and generating power and that there might be a maximum power that can be achieved by the wind turbine regardless of how high the wind can go. Once the turbine is in motion, little is required to keep it going. Some of the power generated might be consumed by the turbine itself at low levels to get it started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %matplotlib inline\n",
    "\n",
    "    # plot the histograms of Speed values\n",
    "    f, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.histplot(data=df, x=\"speed\", ax=axs[0], bins=20, kde=True,color=\"blue\")\n",
    "    sns.histplot(data=df, x=\"power\", alpha=.8, legend=False, ax=axs[1], bins=20, kde=True, color=\"purple\")\n",
    "\n",
    "    #plt.title(\"Speed vs Power\");\n",
    "    plt.suptitle(\"The distribution of the variables\")\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics of the dataset\n",
    "When looking at quantitative variables such as wind speed and power values, the characteristics of interest are the centre, spread, modality (the number of peaks in the pdf), the shape of the distribution and the outliers.\n",
    "Panda's `describe` function generates statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution. (excluding NaN values)\n",
    "The central tendency or location of the data distribution is determined by the typical or middle values. While the mean value is the average value in the dataset it may not be typical of the values in the dataset if there are very small or very large values in the dataset. The median is another measure of central tendancy - it is the middle value after all the values are put in an ordered list.\n",
    "The mean and median are similar for symmetric distributions whereas for unimodal skewed distributions the mean will be more in the direction of the long tail of the distribution. \n",
    "The median can be considered a more typical value in the dataset or closer to some of the typical values and is also considered [robust](https://en.wikipedia.org/wiki/Robust_statistics) which means that removing some of the data will not tend to change the value of the median. A few extreme values will not affect the median as they would affect the mean. \n",
    "In this dataset the mean and median wind speed values are similar at approx 12.5 to 12.6.\n",
    "The median power value is just over 41 compared to the mean power value of 48.\n",
    "As we saw above, there are many zero values for power in the dataset. At least 10% of the power values are zero and probably for valid reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary statistics of the numerical values, \n",
    "df.describe() # get statistics summary of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean speed value is %.3f\" %df['speed'].mean(),\"while the median speed value  is %.3f\" %df['speed'].quantile(q=0.5))\n",
    "print(\"The mean power value is %.3f\" %df['power'].mean(),\"while the median power value  is %.3f\" %df['power'].quantile(q=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spread statistics\n",
    "The variance and standard deviation statistics can be used to show the spread of the distribution of the speed and power data values and how far away from the centre the data points are located.\n",
    "The variance is the average of the squared deviations of each observation from the centre or mean of the data while the standard deviation is the square root of the variance and is in the same units as the data and therefore can be more easily interpreted. \n",
    "The range of values in the data is shown by the minimum and maximum values and is not considered a robust measure of spread but it is useful for showing possible errors or outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"The variance and standard deviations of speed values  are {df['speed'].var():.3f} and {df['speed'].std():.3f}\")\n",
    "#print(f\"The variance and standard deviations of power values are {df['power'].var():.3f} and {df['power'].std():.3f}\")\n",
    "print(f\"The standard deviations of speed values  is {df['speed'].std():.3f}\")\n",
    "print(f\"The standard deviations of power values is {df['power'].std():.3f}\")\n",
    "print(f\"The minimum speed value is {df['speed'].min()} while the maximum speed value is { df['speed'].max()} giving range of {df['speed'].max() - df['speed'].min()}\")\n",
    "print(f\"The minimum Power value is {df['power'].min()} while the maximum power value is { df['power'].max()} giving range of {df['power'].max() - df['power'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentiles or quartiles of the speed and power values can be used to see the spread of the data values. The Interquartile range (IQR) is calculated by taking the 75% percentile or 3rd quartile minus the 25% percentile or first quartile and captures half of the values, the middle values of the data. Data that is more spread out will have a higher IQR. The IQR is considered a more robust measure of spread than the variance and standard deviation and will be more clearly shown in the boxplots further down. The IQR does not consider the data below the 25% percentile or above the 75% percentile which may contain outliers. The statistics here show that the *power* variable in this dataset is much more spread out or variable than the *speed* variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The median speed value is {df['speed'].quantile(q=0.5)} with the IQR ranging from {df['speed'].quantile(q=0.25):.2f} to  {df['speed'].quantile(q=0.75):.2f}\")\n",
    "print(f\"The median power value is {df['power'].quantile(q=0.5)} with the IQR ranging from {df['power'].quantile(q=0.25):.2f} to  {df['power'].quantile(q=0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.set(style=\"ticks\", palette=\"pastel\")\n",
    "sns.boxplot(y=df['speed'], ax=axes[0], color=\"blue\")\n",
    "# add a title\n",
    "axes[0].set_title(\"Boxplot of Speed values\")\n",
    "sns.boxplot(y=df['power'], ax=axes[1], color=\"purple\")\n",
    "axes[1].set_title(\"Boxplot of Power Values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness of the data is a measure of assymetry which can be seen by the lopsidedness of a boxplot.\n",
    "Wind speed appears to be quite symmetric. The wind speed boxplot is cut pretty much in half by the median. Power appears to be soemwhat skewed to the right as the boxplot shows more of the box to the right or above the median line.\n",
    "A boxplot with the median closer to the lower quartile is considered positively skewed. Positively skewed data has the mean greater than the median and it can be interpreted as having a higher frequency of high valued scores. The lower values of power are closer together than the higher power values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating a relationship between Wind Speed and Wind Turbine Power output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots are useful to identify trends and patterns in a dataset which might indicate a relationship between variables. The dataset here contains two numerical variables wind speed and turbine power values. \n",
    "The ordered pairs of numbers consisting of the independent variable wind 'speed' and the dependent variable 'power' output are plotted below resulting in a joint distribution of the two variables. Each point represents an actual observation is the dataset with a speed and a corresponding power value. \n",
    "The scatter plot shows an increasing linear trend in the middle range of the wind speed values. This would indicate that for increasing wind speeds in this range, power output values do increase, but only after a minimum wind speed has been reached. Power outputs then increase in line with increases in wind speed until it reaches a peak and plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Plot size.\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "sns.scatterplot(x=df['speed'],y=df['power'])\n",
    "plt.grid(True)\n",
    "# add title\n",
    "plt.title(\"Scatter plot of Speed and Power\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and Covariance of Speed and Power values\n",
    " \n",
    "For two quantitative variables such as the wind speed and power values, the covariance and correlation can also be used to determine whether a linear relationship between variables does exist and to show if one variable tends to occur with large or small values of another variable. The correlation statistics puts a numerical value on the strength and direction of the relationship. The scatter plot shows a non-linear relationship between the variables. There appears to be a curved pattern in the data so the statistic cannot be taken on it's own.\n",
    "\n",
    "\n",
    "[Covariance](https://en.wikipedia.org/wiki/Covariance) is a measure of the joint variability of two random variables and the (Pearson) [correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is the normalised version of the covariance which shows by its magnitude the strength of the linear relation. The covariance can be used to see how much two variables such as wind speed and wind turbine power vary with each other and in what direction turbine power output will move when wind speed value moves. \n",
    "If the covariance here is positive it would mean that when the wind speed measurement is above it's mean then the wind turbine power measurement will more than likely be above it's mean also and vice versa. \n",
    "If the covarince here is negative, when wind speed measurements are above their mean value, then the wind turbine power output is likely to be below its mean value.\n",
    "If there is a zero covariance (or a covariance value close to zero) this implies that the two variables wind speed and wind turbine output vary independently of each other. \n",
    "\n",
    "The correlation statistics are computed from pairs of arguments. If there is a strong positive relationship between the wind speed and wind turbine power output variables, we would expect to see a correlation coefficient close to 1. A strong negative relationship would have a correlation coefficient value close to -1. A value close to zero would indicate that there is no relationship between the variables. The correlation is easier to interpret than the covariance as it does not depend on the units of measurement or which variable is the independent variable and which is the dependent variable. \n",
    "\n",
    "Pandas has functions for calculating the covariance and correlation coefficients. The correlation coefficient here of 0.85 shows there is a very strong positive relationship between the wind speed and turbine power output. It should be noted that correlation however is not the same as causation.\n",
    "\n",
    "The plots show that the relationship is non-linear though. Maybe look at rand correlation.\n",
    "https://realpython.com/numpy-scipy-pandas-correlation-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "print(scipy.stats.pearsonr(df['speed'], df['power']))\n",
    "print(scipy.stats.spearmanr(df['speed'], df['power']))\n",
    "scipy.stats.kendalltau(df['speed'], df['power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov()\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression plots\n",
    "The Python Seaborn library has some regression plots that can be used to quickly visualise relationships and patterns that may exist in the data. They use statistical models to estimate a simple relationship between sets of observations and are mainly used to visualise patterns in a dataset during the exploratory data analysis.\n",
    "The scatter plot earlier showed a relationship between wind speeds and wind turbine power that is non-linear. There does seem to be a somewhat linear relationship for wind speeds between values of about 10 up to about 18 or so. Therefore it is worth looking at higher order polynomials as well as linear regression.\n",
    "The plot below shows that the polynomial with order 3 looks a much better fit to the line than the first or second order linear regression lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "x = \"speed\"\n",
    "y = \"power\"\n",
    "sns.regplot(x=\"speed\", y=\"power\", data=df, ax=axes[0,0], label=\"order = 1\", ci=False); axes[0,0].legend()\n",
    "sns.regplot(x=\"speed\", y=\"power\", data=df, order=2, ax=axes[0,1], label=\"order =2\", ci=False); axes[0,1].legend()\n",
    "sns.regplot(x=\"speed\", y=\"power\", data=df, order=3, ax=axes[1,0], label=\"order =3\", ci=False); axes[1,0].legend()\n",
    "sns.regplot(x=\"speed\", y=\"power\", data=df, order=4, ax=axes[1,1], label = \"order=4\"); axes[1,1].legend()\n",
    "plt.legend()\n",
    "plt.suptitle(\"Trying higher order polynomial regression functions to Speed and Power values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots can be used to check whether the simple regression model of speed ~ power  is appropriate for a dataset. The seaborn `residplot` fits and removes a simple linear regression and then plots the residual values for each observation. Ideally, these values should be randomly scattered around y = 0. If there is structure in the residuals, this suggests that simple linear regression is not appropriate for the data.\n",
    "\n",
    "The residual plot here does have a shape which suggest non-linearity in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.residplot(x=\"speed\", y=\"power\", data=df, scatter_kws={\"s\": 80})\n",
    "plt.title(\"Residual plot\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Researching Wind Turbines\n",
    "### Drop most of this, just to explain the S-curve and the reasons for the zero values.\n",
    "\n",
    "The dataset consists of two columns each containing 500 floating numbers under the column names `speed` and `power`. There are no other features provided.  I'm not getting into too much specifics about wind turbines but some background would help in understanding the dataset and the relationship between wind speed and power output.\n",
    "The background to the project is related to wind farms supplying electricity to the supply grid and negotiating prices in advance.  In the electricity market, producers of electricity usually sell their electricity ahead of time and enter into a contract where they agree to produce a certain number of kilowatts of electricity during at a certain time, maybe between 11am one day and the next. The price is negotiated in advance of generating electricity and pushing it onto the supply grid. \n",
    "It is easier for those burning coal or gas or doing nuclear fusion to burn the right amount to generate the right amount of electricity. However with wind turbines you cannot tell how much electricity you will generate tomorrow because your generation of electricity depends on wind power. Therefore you can make an estimate by getting some meterological data from a weather prediction agent such as Met Eireann and use the weather prediction to help you predict how much electricity you are likely to produce. The prediction may not always be correct but you aim to reduce how often your calculations are wrong. The aim is to be able to predict that when wind speed is X amount that the power produced from the turbines is Y amount. \n",
    "\n",
    "A little research here suggests that the wind speed values are measured in metres per second and that the power values are measured in kilowatts.\n",
    "\n",
    "[The Irish Wind Energy Association (IWEA)](https://www.iwea.com/about-us/about-us) is the representative body for the Irish wind industry, working to promote wind energy as an essential, economical and environmentally friendly part of the country’s low-carbon energy future. They note here that in 2018 wind energy provided 29 per cent of Ireland’s electricity. Each quarter, both EirGrid and ESBN publish updated wind farm statistics for Ireland at [ESBN Connected Wind Farms](https://www.esbnetworks.ie/new-connections/generator-connections/generator-connection-statistics). There is currently 4,130 MW of installed capacity in ROI and new Eirgrid link here http://www.eirgridgroup.com/how-the-grid-works/renewables/\n",
    "\n",
    "Also on this website is a question about how much electricity a turbine generates. (See FAQ 16.)\n",
    ">The amount of electricity a turbine can generate depends on the type of turbine and the wind conditions at any time. There are many different models of turbines that can generate different amounts of electricity. Ireland’s largest wind farm is the Galway Wind Park in Connemara. The turbines there are 3 MW turbines. To use them as an example, when the wind is blowing steadily they can each generate 3 MW of electricity. A megawatt (MW) is a unit of energy.A single megawatt is equivalent to around 1,000 kilowatts. Boiling a kettle, for example, takes around two kilowatts.\n",
    "\n",
    "Another website of interest is Eirgrid's [Smart grid dashboard](http://smartgriddashboard.eirgrid.com/#all/about).\n",
    "The [dashboard](http://smartgriddashboard.eirgrid.com/#all/wind)shows actual and forecast wind generation by day, week and month for allwind farms on the system. [sem-o.com](https://www.sem-o.com/links/) also some links as well as Ireland's open data portal at  data.gov.ie and the [EU Open data portal](https://data.europa.eu)\n",
    "\n",
    "[WindEurope](https://windeurope.org/data-and-analysis/) has facts and issues about wind energy in Europe, in particular the section on [Wind Energy Basics](https://windeurope.org/about-wind/wind-basics/). Wind is caused by three things, the heating of the atmosphere by the sun, the rotation of the Earth and the Earth's surface irregularities. *air under high pressure moves toward areas of low pressure – and the greater the difference in pressure, the faster the air flows and the stronger the wind!* Energy is the ability to  do work and can be categorised into  either kinetic energy (the energy of moving objects)\n",
    "or potential energy (energy that is stored). Wind turbines take the kinetic energy that’s in the wind and convert that kinetic energy into mechanical power which is mostly used in the form of electricity.\n",
    "Wind energy captures the energy of the wind and converts it to electricity and is an alternative to burning fossil fuels. It comes from a natural and renewable resourse and it is clean as it produces no greenhouse gas, emits no air pollutants and uses very little water. \n",
    "A wind turbine is a device that converts kinetic energy from the wind into electricity. Their output ranges from as small as 100 kilowatts to as big as 12 megawatts.\n",
    "According to this website, there are three main variables determining how much electricity a turbine can produce,  wind speed, blade radius and air density. (We only have the values for wind speed in the dataset here so will have to assume that the other variables are constant)\n",
    "Stronger winds allow more electricity to be produced with higher turbines being more receptive to strong winds. **Wind turbines generate electricity at wind speeds of 4 – 25 metres per second** [123][Wind Energy Basics](https://windeurope.org/about-wind/wind-basics/)\n",
    "\n",
    "The article also outlines what happens when the wind doesn't blow. A wind farms location is usually chosen purposely and therefore when a wind turbine is not turning it is usually because of maintenance, or because it must be stopped for safety reasons in the case of strong winds or a storm. So safety and maintenances reasons do explain the zero power values we see in the data set at the very high values of wind. \n",
    "The article does note that while sometimes there might not be enough wind to turn a turbine, the wind energy is not lost as the wind energy can be stored  in energy storage systems  for later use whenever wind levels are low.\n",
    "I must look at this again - this could explain why there might be some higher values of power that look unusual compared to the norm for those levels of wind.\n",
    "\n",
    "Another article looks at [how to calculate power output of wind](https://www.windpowerengineering.com/calculate-wind-power-output/) and notes that most *U.S. manufacturers rate their turbines by the amount of power they can safely produce at a particular wind speed, usually chosen between 24 mph or 10.5 m/s and 36 mph or 16 m/s.*\n",
    "This indicates that the power values in the dataset are in metres per second.\n",
    "The article gives a formula that illustrates the factors important to the performance of a wind turbine and notes that the wind speed `V` has an exponent of 3 applied to it meaning that even small increases in wind speeds result in a large increase in power.\n",
    "$Power=k.Cp \\frac{1}{2}\\rho AV^3$  where P = power output in kilowatts, Cp = Maximum power coefficient, $\\rho$ is Air density, A = Rotor swept area, V = wind speed in mph, k = 0.000133 a constant to yield power in kilowatts.\n",
    "\n",
    "Additionally the article notes that although the calculation of wind power illustrates important features about wind turbines, the best measure of wind turbine performance is annual energy output. **The difference between power and energy is that power (kilowatts kW) is the rate at which electricity is consumed, while energy (kilowatt-hours kWh) is the quantity consumed.**\n",
    "I'm not sure if this makes a difference here for this project.\n",
    "\n",
    "A blog post [calculating energy production from weather forecast in Python](https://medium.com/planet-os/calculating-energy-production-from-weather-forecast-in-python-3c990047daa) on medium.com use a linear equation to calculate ideal wind production. They do note that modeling results can be enhanced via statistical analysis of hyper-local time series such as meteorological data and energy production data but for this project we do not have any of this data available.\n",
    "Each wind turbine manufacturer provides an ideal energy production curve for their turbines.\n",
    "The article provides a brief overview which is taken from another article [Wind Turbine Power Curve Modeling Using\n",
    "Advanced Parametric and Nonparametric Methods](http://ieeexplore.ieee.org/iel7/5165391/5433168/06894235.pdf):\n",
    "\n",
    "Of particular relevance to this projects is that a typical wind turbine has three main characteristic speeds, the cut-in speeds (Vc), rated speeds (Vr) and cut-out (Vs) speeds.  This helps explain the s-shaped curve we see for this dataset.\n",
    "- The turbine starts generating power when the wind speed reaches the cut-in value.\n",
    "- The rated speed is the wind speed at which the generator is producing the machine’s rated power.\n",
    "- The power generation is shut down to prevent defects and damage when the wind speed reaches the cut-out speed.\n",
    "Also the differences between the ideal power curves which just describe the potentially maximum output versus reality taking into account many factors as well as measurement variations.\n",
    "*In practice, however, wind turbines are never used under ideal conditions, and the empirical power curves could be substantially different from the theoretical ones due to the location of the turbine, air density, wind velocity distribution, wind direction, mechanical and control issues, as well as uncertainties in measurements.* [1234][Wind Turbine Power Curve Modeling Using\n",
    "Advanced Parametric and Nonparametric Methods](http://ieeexplore.ieee.org/iel7/5165391/5433168/06894235.pdf) linked to from the article on medium.com.\n",
    "(I'm just making notes that may be relevant to this project)\n",
    "While the ideal power curves simply describe potentially maximum output, accuracy could be improved by using more accurate accurate weather forecasts or various statistical methods such as machine learning. As in their case we do not have  historical data which we could use for statistical improvements but nor do we have hyper-local forecasts  as we don't know anything about the data.\n",
    "\n",
    "[Using_machine_learning_to_predict_wind_turbine_power_output](https://www.researchgate.net/publication/257748412_Using_machine_learning_to_predict_wind_turbine_power_output)\n",
    "This article notes that real wind turbines do not achieve their theoretical limit as their performance is a function to of aerodynamics and the need to limit power capture once the rated generator power is reached, at ‘rated’ windspeed. \n",
    ">The generator power, turbine diameter and bladeshape are optimized based on site characteristics such as annual average wind speed and the wind speed distribution. \n",
    "\n",
    "Turbine manufacturers measure their turbine’s ‘powercurve’ (the relationship between power output and windspeed) at turbine test sites where it is calculated from 10 min averaged wind speed ($U=\\bar{\\mu}$) and power.\n",
    "*The typical power curves have an s-shape where at wind speeds less than rated the energy capture is approximately proportional to $U^3$ (known as Region II). At wind speeds above rated, the bladepitch and generator torque are actively controlled to limit the power to the generator’s rated power (Region III).*\n",
    "\n",
    "Variations in atmospheric conditions can lead to changes in turbine power output of 10% or more at the same wind speed. Turbulence and shear and not usually used in the power curves as they are considered  difficult to include in turbine power predictions. The article also mentions that because of intermittency in the wind, wind turbines typically produce 20%–40% of their maximum possible output over the course of a year. There is a lot of uncertainty  in predicting power generation from a turbine using local wind speed data. If the amount of energy is overestimated then the site might not be as profitable as expected while underestimating the energy available at a site might lead to a site not being developed at all. This study simulated the aerodynamic forces on the turbines blades and structures using an aerostructuraland simulator and  created 1796 10 minute wind fields from a stochastic turbulence simulator. The data from the wind fields from the simulations were used to form a database of 1796 observations of 10 min average power (the response) as a function of wind speed, turbulence intensity, and shear exponent (the forcing). \n",
    "\n",
    "They binned the power data into 1 m $s^1$ wide bins and included a plot (figure 3 in the article) of the power curve which shows a Region II between 0.3 metres per second and about 11.5 metres per second, region III is from end the end of region II up to 25 which correspond exactly with our dataset.\n",
    "(The power is in kilowatts though with max value of 1500 so maybe I have the wrong metrics here.)\n",
    "\n",
    "Noted that *although the forcing variables are evenly distributed, variance in power is largest near rated wind speed. This sensitivity may result in large variation between predicted power output and observed power output. Furthermore, the mean power generated in simulations that include turbulence is lower than the no-turbulence cases near rated wind speed.*\n",
    "### Will be removing this mostly.\n",
    "#### wind speeds below 8 metres per second, between 7 and 8 metres per second\n",
    "At wind speeds below 8 m $s^{-1}$, power increases with turbulence intensity and shear. The increase in power due to turbulence arises because turbulent flow with mean speed $U$ carries more power than laminar flow of the same $U$. The changes in power output of +/-20% associated with turbulence are approximately half of the change due to a change in wind speed from 7 to 8 $m^{s−1}$.\n",
    "In contrast, at wind speeds just above and below rated speed, increasing turbulence intensity reduces power output as the turbine cannot capture the extra energy that gusts bring, but a short duration slow down to wind speeds below rated results in a loss of energy. As the mean wind speed increases, the total amount of time with the blades pitched toward feather increases and the wind turbine is more often operating at rated power. At wind speeds much greater than rated, larger turbulence intensities are required to reduce the output of the machine to less than rated power, regardless of shear. In Regions II and III, variation in $Ti$ impacts power performance more than variation in $\\alpha$.\n",
    "\n",
    "The article next looked at using machine learning to predict power output under different conditions by incorporating turbulence intensity and wind shear into power prediction tools. They note that the power output from the turbine is not a linear function of wind speed and so linear regression is not an appropriate technique. Non-linear regression assumes that the relationships are constant throughout the model space (i.e. power output is always proportional to $U^n$ ), which is incorrect, so non-linear regression is also inappropriate. Also, as multivariate bins only work where the training data includes data in all bins this would be computationally or observationally more expensive. Instead they chose a machine learning technique `regression trees` for capturing non-linear changes in response to forcing. Regression trees are models that use simple branching question paths to predict an outcome based on inputs. \n",
    "\n",
    "The turbines responds differently to changes in shear and turbulence at different wind speeds. In Region II, at wind speeds below 8 m $s^−1$, power output increases by up to 10% as turbulence increases or as the magnitude of the shear increases. At wind speeds greater than 8 m $s^−1$ and in Region III, the regression tree modeled power is consistent with the simulated power output: power decreases as turbulence intensity increases and shows weak or no dependence on shear.\n",
    "\n",
    "The article concludes:\n",
    ">simulations suggest and the model clearly demonstrates that the response of the turbine is a complex non-linear function of hub height wind speed, turbulence intensity, and rotor disk shear. At wind speeds below rated speed, the turbine power output is most sensitive to changes in wind speed and speed, turbulence. At rated speed, the turbine is most sensitive to turbulence intensity and shear, and power can change by 10% under typical atmospheric conditions. At wind speeds greater than rated, the turbine responds most to changes in turbulence intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rated wind speeds\n",
    "https://www.sciencedirect.com/topics/engineering/rated-wind-speed \n",
    "https://www.sciencedirect.com/topics/engineering/rated-wind-speed\n",
    "\n",
    "https://www.wind-watch.org/faq-technology.php\n",
    "Here are a few points from this site although the wind turbines are probably not the same turbines as our dataset. \n",
    "The article earlier showed the power curves for different utility-scale wind turbines which all reach higher maximum powers than the values in this dataset. (Using_machine_learning_to_predict_wind_turbine_power_output)(https://www.researchgate.net/publication/257748412_Using_machine_learning_to_predict_wind_turbine_power_output)\n",
    "\n",
    "The cut-in speeds quoted here are in miles per hour. (to convert miles per hour to metres per second divide by 2.237).\n",
    "\n",
    "- Every wind turbine design has a cut-in wind speed, a rated wind speed, and a cut-out wind speed.\n",
    "- At the cut-in wind speed, the blades start to turn and a trickle of electricity starts to be produced. Around cut-in, the generator may be used as a motor to help the wind overcome inertia and start the blades turning.\n",
    "- The cut-in speed is typically 7 to 9 mph. (equivalent to 3.13 to 4.02 meters per second)\n",
    "- At the rated wind speed, the turbine is able to generate electricity at its maximum, or rated, capacity.\n",
    "- The rated speed is usually in the range of 25 to 35 mph. (equivalent to 11.18 to 13.4 metres per second).\n",
    "\n",
    "- At the cut-out wind speed, the turbine shuts down to avoid damage. The pitch controllers feather the blades to let the wind flow past them and the rotor hub is braked. The wind usually has to return to a much lower speed, called the cut-back-in wind speed, for a certain amount of time before the turbine will restart.\n",
    "- The cut-out speed is generally around 55 mph (24.586 metres per second). The cut-back-in speed is around 45 mph (20.12 metres per second).\n",
    "\n",
    "The rated windspeed of a wind turbine is the wind speed at which a turbine hits its maximum “rated\" or “nameplate\" capacity\" power output.\n",
    "https://www.quora.com/What-do-you-mean-by-the-rated-wind-speed-of-the-wind-turbine\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Wind_turbine_design\n",
    ">A wind turbine is designed to produce power over a range of wind speeds. The cut-in speed is around 3–4 m/s for most turbines, and cut-out at 25 m/s. If the rated wind speed is exceeded the power has to be limited. There are various ways to achieve this.\n",
    ">All wind turbines are designed for a maximum wind speed, called the survival speed, above which they will be damaged. The survival speed of commercial wind turbines is in the range of 40 m/s (144 km/h, 89 MPH) to 72 m/s (259 km/h, 161 MPH). The most common survival speed is 60 m/s (216 km/h, 134 MPH). Some have been designed to survive 80 metres per second (290 km/h; 180 mph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 mph or 10.5 m/s and 36 mph or 16 m/s\n",
    "To convert miles per hours to metres per second divide the speed value by 2.237\n",
    "https://www.metric-conversions.org/speed/miles-per-hour-to-meters-per-second.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does all this tie in with our dataset?\n",
    "\n",
    "The scatter plot of our dataset did show three distinct sections of the curve, the first section at low values of wind speed where the wind turbine values are clustered around zero (including the 10% of observations that have zero power values), the second section where there seems to be a linear trend between wind speed and power output and thirdly the last section where the values of power have reached a peak at higher wind speeds and plateau, and also the outliers here.\n",
    "We would expect to see power being generated when speed is above 3-4 metres per second.\n",
    "Cut out speed at 24-25 metres per second.\n",
    "Cut back in speed after cut out is 20 meters per second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just using clustering for exploratory data analysis, to see if the data is clustered into three regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning - KMeans.\n",
    "import sklearn.cluster as skcl\n",
    "x1 = np.array(df.speed)\n",
    "x2 = np.array(df.power)\n",
    "X =  np.vstack([x1, x2]).T\n",
    "X.size \n",
    "# 1000\n",
    "X.shape\n",
    "# (500, 2)\n",
    "# Plots styles.\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Plot size.\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "# Plot the data set.\n",
    "\n",
    "# Plot the data set.\n",
    "plt.plot(X[:,0], X[:,1], 'x')\n",
    "\n",
    "# Perform kmeans fitting.\n",
    "kmeans = skcl.KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "plt.plot(X[kmeans.labels_ == 0][:,0], X[kmeans.labels_ == 0][:,1], 'gx')\n",
    "plt.plot(X[kmeans.labels_ == 1][:,0], X[kmeans.labels_ == 1][:,1], 'rx')\n",
    "plt.plot(X[kmeans.labels_ == 2][:,0], X[kmeans.labels_ == 2][:,1], 'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='speed', ascending=False).head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# show the unique values\n",
    "df.speed.unique()\n",
    "# a count of the unique speed values\n",
    "df.speed.value_counts() # 490\n",
    "# a count of the unique power values\n",
    "df.power.value_counts() # 451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.groupby('speed')) # 490\n",
    "df.groupby('speed')['power'].agg(['count', 'size', 'nunique'])\n",
    "len(df.groupby('power')) # 451\n",
    "\n",
    "df.groupby('power')['speed'].agg(['count', 'size', 'nunique'])\n",
    "len(df[['speed', 'power']].drop_duplicates()) # 490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero values\n",
    "#### Where do the zero values occur?\n",
    "- There are 49 datapoints where power value is zero\n",
    "- There is only 1 datapoint where speed value is zero (also power)\n",
    "- Ten speed values over 24.4 metres per hour with a corresponding power value of zero\n",
    "- There are no data points with speeds above 24.4 and power not equal to zero\n",
    "- Eight data points where speed is less than 8 metres per second and power value is zero\n",
    "- Four datapoints where speed is greater than 8 and less than 24.4 metres per second and corresponding power values are zero\n",
    "- 24 datapoints where speed is less than 4 metres per second and power is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the rows where both values are zer0\n",
    "print(df[(df['speed']==0) & (df['power']==0)].count()) # only 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 49 observations with zero power values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.power==0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cut-in speeds between 3 and 4 metres per second\n",
    "The cut-in speed is typically 7 to 9 mph. (equivalent to 3.13 to 4.02 meters per second).\n",
    "- 17 observations where speed is less than 3 metres per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Zero power values where speed is below 3 metres per second:\\n{df[(df.speed <3) & (df.power==0.0)].count()}\")\n",
    "print(f\"Zero power values where speed is between 3 and 4 metres per second:\\n{df[(df.speed >3) & (df.speed <4) & (df.power==0.0)].count()}\")\n",
    "print(f\"Zero power values where speed is between 4 and 7 metres per second:\\n{df[(df.speed >4) & (df.speed<7) & (df.power==0.0)].count()}\")\n",
    "print(f\"Zero power values where speed is between 7 and 24.4 metres per second:\\n{df[(df.speed >7) & (df.speed<24.4) & (df.power==0.0)].count()}\")\n",
    "print(f\"Zero power values where speed is above 24.4 metres per second:\\n{df[(df.speed>24.4) & (df.power==0.0)].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-zero power values where speed is less than 3 metres per second:\n",
    "There are 39 observations where speed is less than the cut-in speed (of between 3 and 4 metres per second) and the corresponding power value is greater than zero. I think these are the tricky ones to predict. There was mention in the one of the articles referenced earlier that while sometimes there might not be enough wind to turn a turbine, the wind energy is not lost as the wind energy can be stored in energy storage systems for later use whenever wind levels are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.speed < 3) & (df.power>0.0)].count() # 39\n",
    "df[(df.speed < 3) & (df.power==0.0)].count() # 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.speed < 4) & (df.power>0.0)].count() # 56\n",
    "df[(df.speed < 4) & (df.power==0.0)].count() # 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rated speed\n",
    "At the rated wind speed, the turbine is able to generate electricity at its maximum, or rated, capacity.\n",
    "The rated speed is usually in the range of 25 to 35 mph. (equivalent to 11.18 to 13.4 metres per second).\n",
    "This is based on the research above but we don't know the details of the actual wind turbines being used here.\n",
    "The rated speed for the turbines in this dataset must be greated than this range as they do not reach their maximum capacity until wind speed is greater than 15 metres per second.\n",
    "The max power value in the dataset is 113.556 kws but there are very few observations in the dataset where power is greater than 110. The power curve for this dataset shows the power values levelling off in and around values between 90 and 100. I need to blow the plot up some more to see the exact values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power values greater than 100\n",
    "- 31 power values between 80 and 90\n",
    "- 95 power values between 90 and 100\n",
    "- 55  between 100 and 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data points where power is between 80 and 90:\\n{df[(df.power>80)&(df.power<90)].count()}\")\n",
    "print(f\"Data points where power is between 90 and 100:\\n{df[(df.power>90)&(df.power<100)].count()}\")\n",
    "print(f\"Data points where power is between 100 and 110:\\n{df[(df.power>100)&(df.power<110)].count()}\")\n",
    "print(f\"Data points where power greater than 110:\\n{df[(df.power>110)].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.power>100].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.speed >11) & (df.speed< 13.4)].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cut-out speeds for safety reasons\n",
    "At the cut-out wind speed, the turbine shuts down to avoid damage.\n",
    "There are 10 observations that fall into the cut-out speed range.\n",
    "There are no datapoints where speed is above the cut-out speed value where power is not zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no points in the dataset where speed is greater than 24.4 and power value is not zero\n",
    "print(f\"Speed values above 24.4 metres per second:\\n{df[df.speed>24.4].count()}\") # \n",
    "print(f\"Zero power values where speed is above 24.4 metres per second:\\n{df[(df.speed > 24.4) & (df.power==0.0)].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speeds between 7 and 8 metres per second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.speed >7) & (df.speed < 8) & (df.power==0.0)].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with zero values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After initially trying out the neural network using the complete dataset, the cost did not fall below a certain value and the resulting plots suggested that keeping the zero power values in for the very high values of speed were affecting the calculations. The research showed that there is a cut-out speed between 24 and 25 metres per second for safety reasons.  At the cut-out wind speed, the turbine shuts down to avoid damage. This is enough to justify excluding these observations as we can predict that the power output will be zero when the wind speed exceeds this cut out value. There are only ten observations in the dataset that fall into this range.\n",
    "We can only predict values for power when the turbines are turned on and therefore maybe the model should only be predicting for values of speed where the turbine is on! \n",
    "\n",
    "While there is only one zero value for the speed variable, there are 49 zero values for the power variable. These mostly occur below a certain value of speed but located alongside non-zero power values and there are a few that are associated with medium and higher speed values of speed. Most of the data points in the dataset are unique values. The one datapoint with a zero speed value has a zero power value as expected.\n",
    "\n",
    "Summary of where the zero power values occur:\n",
    "- 17 where speed is less than 3 metres per second\n",
    "- 7 where speed is between 3 and 4 metres per second. This is the cut-in speed\n",
    "- 9 where speed lies between 4 and 7\n",
    "- 6 where speed is between 7 and 24.4\n",
    "- 10 where speed is above 24.4 metres per second. This is the cut-out value.\n",
    "\n",
    "For now I will drop all observations where speed is greater than the cut-out value of 24.4. \n",
    "I will also drop the observations above the cut-in speed of between 3 and 4 metres per second.\n",
    "I will leave in all the observations where speed is less than the cut-in speed including the zero values.\n",
    "(Wind turbines generate electricity at wind speeds of 4 – 25 metres per second [123]Wind Energy Basics)\n",
    "\n",
    "This means that for now I am dropping 25 observations from the dataset where the power values are zero.\n",
    "Ten observations where the wind speed is greater than the cut-off value; the corresponding power is zero as the turbines are off.\n",
    "Fifteen observations where wind speed is greater than the cut-in and less than the cut-out and the power is zero. Assume these represent points where the turbines are turned off for maintenance or other reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "# https://thispointer.com/python-pandas-how-to-drop-rows-in-dataframe-by-conditions-on-column-values/\n",
    "#df.drop(df.loc[(df.speed>24.4)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe\n",
    "dfx = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed    25\n",
       "power    25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.loc[(dfx.speed > 4)&(dfx.power == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.drop(dfx.loc[(df.speed > 4)&(dfx.power == 0)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.440971</td>\n",
       "      <td>50.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.101231</td>\n",
       "      <td>41.171815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.293500</td>\n",
       "      <td>6.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.538000</td>\n",
       "      <td>47.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.531500</td>\n",
       "      <td>94.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.399000</td>\n",
       "      <td>113.556000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            speed       power\n",
       "count  475.000000  475.000000\n",
       "mean    12.440971   50.541667\n",
       "std      7.101231   41.171815\n",
       "min      0.000000    0.000000\n",
       "25%      6.293500    6.737500\n",
       "50%     12.538000   47.282000\n",
       "75%     18.531500   94.017500\n",
       "max     24.399000  113.556000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = \"speed\"\n",
    "y = \"power\"\n",
    "\n",
    "sns.regplot(x=\"speed\", y=\"power\", data=dfx, order=4, label=\"order =3\", ci=False)\n",
    "\n",
    "plt.suptitle(\"Trying higher order polynomial regression functions to Speed and Power values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning.\n",
    "\n",
    "Here I am going to start looking at applying some machine learning algorithms to the dataset. \n",
    "\n",
    "The goal of the project is to predict wind power from wind speed and therefore this problem falls into supervised learning. \n",
    ">Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output [Machine Learning Mastery](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/) which is precisely what we have been asked to do here.\n",
    "\n",
    "The algorithms I will start looking at are as follows:\n",
    "\n",
    "- Regression: Predicting a continuous-valued attribute associated with an object.\n",
    "I had a look at the regression plots earlier, simple linear regression does not adequately model the relationship between the wind and speed values over the entire dataset while higher order polynomials captured more of the relationship. The simple linear model clearly underfit the data. The correlation statistics did show a high correlation between the variables with a value of 0.853778 but this doesn't mean the relationship is linear.\n",
    "There were large sections of the dataset that were either under the regression line or above the regression line.\n",
    "The higher order polynomials did not fit the data perfectly though. The cubic polynomial fits the data better and the fourth order polynomial does seem to be even better for the datapoints at the lower end.\n",
    "Now that some of the zero values have been removed I will look at fitting a curve again using a higher order polynomial. The numpy polyfit function can be used for this or alternatively scipy curve_fit function or scikit-learns polynomial regression. \n",
    "Actually I'm leaving this out for now and just including the neural networks.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the science of getting computers to act without being explicitly programmed.\n",
    "- https://www.investopedia.com/terms/n/neuralnetwork.asp#:~:text=A%20neural%20network%20is%20a,organic%20or%20artificial%20in%20nature.\n",
    "- https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "\n",
    "- https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414\n",
    "\n",
    ">Neural nets are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples.\n",
    ">Modeled loosely on the human brain, a neural net consists of thousands or even millions of simple processing nodes that are densely interconnected. Most of today’s neural nets are organized into layers of nodes, and they’re “feed-forward,” meaning that data moves through them in only one direction. An individual node might be connected to several nodes in the layer beneath it, from which it receives data, and several nodes in the layer above it, to which it sends data.\n",
    ">To each of its incoming connections, a node will assign a number known as a “weight.” When the network is active, the node receives a different data item — a different number — over each of its connections and multiplies it by the associated weight. It then adds the resulting products together, yielding a single number. If that number is below a threshold value, the node passes no data to the next layer. If the number exceeds the threshold value, the node “fires,” which in today’s neural nets generally means sending the number — the sum of the weighted inputs — along all its outgoing connections.\n",
    "When a neural net is being trained, all of its weights and thresholds are initially set to random values. Training data is fed to the bottom layer — the input layer — and it passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer. During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs.\n",
    "[Mit News](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414)[nn1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Neural Networks\n",
    "\n",
    "Here I will go through the lecture notes first and apply some of the techniques.\n",
    "Some notes from the lectures:\n",
    "A dataset contains variables, some inputs and outputs. Categorical data, text and image data must first be encoded into floating point numbers. Outputs can be reverse decoded later. Weights are applied to each connection between layers, low weights near 0 meaning that the route is not important while higher values mean this route is important. \n",
    "Some of the dataset can be held back for testing. \n",
    "Neural networks have several layers. The input layer is really a non-layer of neurons with just the inputs. You just provide the input layer to the neural network and this is really the only place the input comes into it.\n",
    "\n",
    "The hidden layers and output layers have proper neurons.\n",
    "The weights and bias are the paramters of the model, you can tweak them to get the neural network to perform better. Often the initial weights might be set to small random values between 0 and 1. \n",
    "Neural networks cares more about the connections between the nodes than the actual nodes themselves. Looks at the signal times the weights for each of the inputs. `w * x` for each input. \n",
    "\n",
    "The bias significes how important something is and is just a number on top of weight times input for each neuron. \n",
    "With neural networks you want to get a high value assigned to the correct output. If the neural network misclassifies an output label then you just feed it back into the algorithm, the weights are changed a little so that the correct output is predicted the next time. It will keep changing until it gets the correct output. \n",
    "Starts out with random values for the weights. These are updated slowly over time to get towards the expected output.\n",
    "\n",
    "\n",
    "Gradient descent is the algorithm through which the weights are updated. Stochastic gradient descent (`sgd`).\n",
    "Each output from each neuron is the same but when the different weights are applied, what is received by the nodes in the next layer are different to what left the nodes on the previous layer.\n",
    "The loss is the cost function, often the MSE mean squared error.\n",
    "\n",
    "A linear function y = x is just a simple function where you give it a single input and expect the same output.\n",
    "\n",
    "Once the neural network is trained and is stable, you can then take a new input, a new datapoint it was not trained on and the neural network should correctly classify it.\n",
    "the complexity in neural networks comes from the number of operations involved especially when there are many layers.\n",
    "\n",
    "---\n",
    "\n",
    "https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "Taking notes from another machine learning mastery tutorial.\n",
    "\n",
    "### The 5-step Deep Learning Model Life-Cycle\n",
    "A model has a life-cycle with 5 steps:\n",
    "1. Define the model: \n",
    "Select the type of model and choose the architecture or network topology. Define the number of layers, configure each layer with a number of nodes and activation function, connect the layers together into a cohesive model.\n",
    "\n",
    "2. Compile the model\n",
    "\n",
    "    * This requires selecting a **loss function** that you want to optimise such as mean squared error and to select an algorithm to perform the **optimisation** procedure such as stochastic gradient descent or a modern variation of this called Adam. Also you can select the **performance metrics**  to keep track of during the model training process.\n",
    "    * Call a function to compile the model with the selected configuration to prepare the appropriate data structures required for the efficient use of the selected model. \n",
    "    * The optimizer can be specified as a string for a known optimizer class such as `sgd` for stochastic gradient descent or else you can configure an instance of the optimizer class to use. See [optimizer classes](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
    "\n",
    "    * `sgd` - Gradient descent (with momentum) optimizer. The default learning rate is 0.01\n",
    "    The update rule is ` w = w - learning_rate * g` for parameter `w` with  gradient `g` when momentum is 0.\n",
    "    * \n",
    "\n",
    "    *Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.* [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam]\n",
    "    *  The `learning_rate` defaults to 0.001 in the Adam optimizer. \n",
    "\n",
    "    - Loss Functions: class MeanSquaredError: Computes the mean of squares of errors between labels and predictions.\n",
    "    The mean squared error is the one I will be using. There are many more listed in the [keras losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses) documentation.\n",
    "    \n",
    "    * Metrics to evaluate predictions such as `Accuracy`  which calculates how often predictions equal labels, `AUC` computes the approximate area under the curve,  `FalseNegatives` which calculates the number of false negatives and `FalsePositives` calculates the number of false positives, `Precision` computes the precision of the predictions with respect to the labels. \n",
    "    There are many more [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) which I will come back when I get to that point...\n",
    "\n",
    "3. Fit the model\n",
    "\n",
    "To fit the model you need to select the training configuration such as the number of epochs which are loops through the training dataset. The batch size is the number of samples in an epoch used to estimate model error.\n",
    "(By passing in say 10 at a time instead of 1 at a time can have a smoothing effect)\n",
    "\n",
    "Training applies the chosen optimization algorithm to minimize the chosen loss function and updates the model using the backpropagation of error algorithm. This can be slow depending on the complexity of the model, the size of the training dataset and the hardware being used.\n",
    "\n",
    "There is another machine learning article here on [the importance of batch sizes](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/)\n",
    "\n",
    "\n",
    "\n",
    "4. Evaluate the model\n",
    "\n",
    "This is where the holdout dataset comes into play, data that is not used in the training of the model so you can get an unbiased estimate of the performance of the model when making predictions on new data.\n",
    "\n",
    "5. Make predictions\n",
    "This is the final stage of the life cycle where you take values that you don't have target values and make a prediction.\n",
    "\n",
    "### Sequential model\n",
    "A **sequential** model adds layers to the model one by one in a linear manner, from input to output.\n",
    "    The visible layer of the network is defined by the input shape argument on the first hidden layer. In the wind dataset this will be `(1,)` - the model will expect the input for one sample to be a vector of 1 number.\n",
    "`model.add()` is used to add each layer.\n",
    "\n",
    "There is also a functional model which is more complex but more flexible. It involves explicitly connecting the output of one layer to the input of another layer. Each connection is specified. I will be using the sequential model for this project.\n",
    "\n",
    "###  Develop Multilayer Perceptron Models\n",
    "\n",
    "A Multilayer Perceptron model (MLP)is a standard fully connected neural network model. It is made up of one or more (dense) layers of nodes where each node is connected to all outputs from the previous layer and the output of each node is connected to all the inputs for the nodes in the next layer. This model is suitable for tabular data and can be used for three predictive modeling problems being binary classification, multiclass classification, and regression. \n",
    "The tutorial demostrates using MLP for each of the three predictive modeling problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Neural networks.\n",
    "import tensorflow.keras as kr\n",
    "print(kr.__version__)\n",
    "\n",
    "# Numerical arrays\n",
    "import numpy as np\n",
    "\n",
    "# Data frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# splitting the data into training and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for regression\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "path = 'https://raw.githubusercontent.com/ianmcloughlin/2020A-machstat-project/master/dataset/powerproduction.csv'\n",
    "data = dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already done this in dfx\n",
    "# drop some of the outliers\n",
    "#df.drop(df.loc[(df.speed>24.4)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.440971</td>\n",
       "      <td>50.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.101231</td>\n",
       "      <td>41.171815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.293500</td>\n",
       "      <td>6.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.538000</td>\n",
       "      <td>47.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.531500</td>\n",
       "      <td>94.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.399000</td>\n",
       "      <td>113.556000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            speed       power\n",
       "count  475.000000  475.000000\n",
       "mean    12.440971   50.541667\n",
       "std      7.101231   41.171815\n",
       "min      0.000000    0.000000\n",
       "25%      6.293500    6.737500\n",
       "50%     12.538000   47.282000\n",
       "75%     18.531500   94.017500\n",
       "max     24.399000  113.556000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data split into training and test sets\n",
    "\n",
    "- df is the original dataset with all 500 datapoints,\n",
    "- dfx is another copy where I dropped some of the the rows with zero power when the speed is over a certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Scikit-learn preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 1) (157, 1) (318, 1) (157, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into input and output columns\n",
    "\n",
    "X, y = dfx.values[:, :-1], dfx.values[:, -1:]\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First without scaling the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(50, input_shape=(1,), activation='sigmoid', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "#model.add(kr.layers.BatchNormalization())\n",
    "model.add(kr.layers.Dense(1, activation='linear', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "model.compile(kr.optimizers.Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 3924.0527 - val_loss: 3859.6675\n",
      "Epoch 2/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3802.3879 - val_loss: 3734.7974\n",
      "Epoch 3/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3677.9858 - val_loss: 3606.2480\n",
      "Epoch 4/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3543.0857 - val_loss: 3456.7388\n",
      "Epoch 5/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3383.4187 - val_loss: 3290.6401\n",
      "Epoch 6/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3215.5803 - val_loss: 3124.2859\n",
      "Epoch 7/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3064.4319 - val_loss: 2985.8936\n",
      "Epoch 8/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2941.0830 - val_loss: 2875.7148\n",
      "Epoch 9/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2839.0769 - val_loss: 2775.2329\n",
      "Epoch 10/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2746.1658 - val_loss: 2686.2341\n",
      "Epoch 11/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2660.7883 - val_loss: 2601.9661\n",
      "Epoch 12/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2577.4985 - val_loss: 2520.1829\n",
      "Epoch 13/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2497.5864 - val_loss: 2439.0293\n",
      "Epoch 14/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2421.6655 - val_loss: 2361.1687\n",
      "Epoch 15/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2346.9153 - val_loss: 2293.9897\n",
      "Epoch 16/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2279.9036 - val_loss: 2224.5928\n",
      "Epoch 17/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2215.1174 - val_loss: 2159.5273\n",
      "Epoch 18/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2153.1021 - val_loss: 2099.1958\n",
      "Epoch 19/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2095.0579 - val_loss: 2039.9879\n",
      "Epoch 20/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2038.8182 - val_loss: 1984.0060\n",
      "Epoch 21/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1984.8591 - val_loss: 1929.2325\n",
      "Epoch 22/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1931.6827 - val_loss: 1877.8844\n",
      "Epoch 23/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1880.7142 - val_loss: 1826.6893\n",
      "Epoch 24/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1831.2249 - val_loss: 1777.4205\n",
      "Epoch 25/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1782.8687 - val_loss: 1729.3267\n",
      "Epoch 26/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1735.9181 - val_loss: 1684.2961\n",
      "Epoch 27/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1690.9739 - val_loss: 1639.1420\n",
      "Epoch 28/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1647.5933 - val_loss: 1595.1884\n",
      "Epoch 29/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1605.2759 - val_loss: 1552.9883\n",
      "Epoch 30/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1564.8351 - val_loss: 1511.3445\n",
      "Epoch 31/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1524.4371 - val_loss: 1473.8406\n",
      "Epoch 32/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1485.8516 - val_loss: 1434.9667\n",
      "Epoch 33/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1447.6100 - val_loss: 1397.6965\n",
      "Epoch 34/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1410.5845 - val_loss: 1360.8807\n",
      "Epoch 35/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1374.6503 - val_loss: 1323.8740\n",
      "Epoch 36/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1339.0548 - val_loss: 1290.0900\n",
      "Epoch 37/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1305.1136 - val_loss: 1255.1373\n",
      "Epoch 38/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1270.8339 - val_loss: 1222.8129\n",
      "Epoch 39/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1238.5138 - val_loss: 1189.1401\n",
      "Epoch 40/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1206.1112 - val_loss: 1158.2904\n",
      "Epoch 41/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1174.6879 - val_loss: 1127.0051\n",
      "Epoch 42/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1144.4907 - val_loss: 1096.9900\n",
      "Epoch 43/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1114.4480 - val_loss: 1068.2087\n",
      "Epoch 44/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1084.7159 - val_loss: 1039.1610\n",
      "Epoch 45/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1057.3894 - val_loss: 1011.1227\n",
      "Epoch 46/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1027.8430 - val_loss: 984.4299\n",
      "Epoch 47/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1000.1741 - val_loss: 956.8706\n",
      "Epoch 48/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 974.1363 - val_loss: 930.3113\n",
      "Epoch 49/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 947.5277 - val_loss: 906.2711\n",
      "Epoch 50/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 922.8056 - val_loss: 880.0809\n",
      "Epoch 51/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 896.7781 - val_loss: 856.4014\n",
      "Epoch 52/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 872.1638 - val_loss: 831.9104\n",
      "Epoch 53/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 848.0691 - val_loss: 808.7607\n",
      "Epoch 54/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 824.3105 - val_loss: 787.1209\n",
      "Epoch 55/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 802.5472 - val_loss: 764.6294\n",
      "Epoch 56/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 779.3356 - val_loss: 743.5843\n",
      "Epoch 57/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 758.5264 - val_loss: 722.1342\n",
      "Epoch 58/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 736.9789 - val_loss: 701.0628\n",
      "Epoch 59/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 716.4494 - val_loss: 681.6944\n",
      "Epoch 60/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 696.3340 - val_loss: 663.1851\n",
      "Epoch 61/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 676.3914 - val_loss: 644.1682\n",
      "Epoch 62/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 658.3930 - val_loss: 625.8141\n",
      "Epoch 63/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 639.4323 - val_loss: 607.2134\n",
      "Epoch 64/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 620.9257 - val_loss: 590.2350\n",
      "Epoch 65/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 603.2529 - val_loss: 574.3040\n",
      "Epoch 66/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 585.9778 - val_loss: 557.7506\n",
      "Epoch 67/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 570.4178 - val_loss: 542.1389\n",
      "Epoch 68/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 553.1620 - val_loss: 526.6292\n",
      "Epoch 69/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 537.5515 - val_loss: 512.1494\n",
      "Epoch 70/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 522.0132 - val_loss: 496.8957\n",
      "Epoch 71/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 507.3446 - val_loss: 483.2639\n",
      "Epoch 72/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 493.0334 - val_loss: 470.6739\n",
      "Epoch 73/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 479.0415 - val_loss: 455.8493\n",
      "Epoch 74/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 465.1257 - val_loss: 442.9536\n",
      "Epoch 75/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 452.0009 - val_loss: 430.8349\n",
      "Epoch 76/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 439.3785 - val_loss: 419.3716\n",
      "Epoch 77/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 426.3115 - val_loss: 406.5449\n",
      "Epoch 78/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 414.8113 - val_loss: 395.4602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 402.8637 - val_loss: 384.2448\n",
      "Epoch 80/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 391.4275 - val_loss: 375.3445\n",
      "Epoch 81/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 380.0105 - val_loss: 363.4444\n",
      "Epoch 82/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 369.9919 - val_loss: 353.5643\n",
      "Epoch 83/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 359.1191 - val_loss: 343.8066\n",
      "Epoch 84/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 349.2335 - val_loss: 334.5268\n",
      "Epoch 85/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 339.5912 - val_loss: 325.2852\n",
      "Epoch 86/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 330.2416 - val_loss: 317.0385\n",
      "Epoch 87/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 320.9366 - val_loss: 308.2330\n",
      "Epoch 88/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 312.8073 - val_loss: 299.8059\n",
      "Epoch 89/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 303.8475 - val_loss: 291.5633\n",
      "Epoch 90/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 295.8883 - val_loss: 284.8515\n",
      "Epoch 91/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 288.0015 - val_loss: 277.0505\n",
      "Epoch 92/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 280.1428 - val_loss: 269.6971\n",
      "Epoch 93/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 272.8248 - val_loss: 262.8967\n",
      "Epoch 94/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 265.9718 - val_loss: 256.5161\n",
      "Epoch 95/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 258.1946 - val_loss: 249.4070\n",
      "Epoch 96/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 251.6680 - val_loss: 243.0424\n",
      "Epoch 97/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 245.2804 - val_loss: 237.3786\n",
      "Epoch 98/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 238.6905 - val_loss: 231.5521\n",
      "Epoch 99/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 232.6576 - val_loss: 225.9304\n",
      "Epoch 100/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 228.1138 - val_loss: 220.2259\n",
      "Epoch 101/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 221.1089 - val_loss: 214.7746\n",
      "Epoch 102/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 215.8647 - val_loss: 210.0037\n",
      "Epoch 103/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 210.4319 - val_loss: 205.3036\n",
      "Epoch 104/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 205.6981 - val_loss: 200.1808\n",
      "Epoch 105/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 201.3751 - val_loss: 195.2450\n",
      "Epoch 106/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 196.1933 - val_loss: 190.9222\n",
      "Epoch 107/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 190.8512 - val_loss: 186.8911\n",
      "Epoch 108/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 186.5701 - val_loss: 182.7391\n",
      "Epoch 109/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 182.4298 - val_loss: 178.6986\n",
      "Epoch 110/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 178.4345 - val_loss: 174.4355\n",
      "Epoch 111/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 174.1791 - val_loss: 170.8862\n",
      "Epoch 112/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 170.2513 - val_loss: 167.2589\n",
      "Epoch 113/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 166.7652 - val_loss: 163.9488\n",
      "Epoch 114/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 163.2979 - val_loss: 160.4200\n",
      "Epoch 115/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 159.7019 - val_loss: 156.9349\n",
      "Epoch 116/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 156.1808 - val_loss: 153.8499\n",
      "Epoch 117/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 153.2392 - val_loss: 150.9661\n",
      "Epoch 118/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 150.0148 - val_loss: 147.8099\n",
      "Epoch 119/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 147.0696 - val_loss: 145.1555\n",
      "Epoch 120/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 144.2476 - val_loss: 142.1918\n",
      "Epoch 121/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 141.2711 - val_loss: 139.5408\n",
      "Epoch 122/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 138.3449 - val_loss: 136.9913\n",
      "Epoch 123/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 135.5018 - val_loss: 134.3992\n",
      "Epoch 124/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 133.4457 - val_loss: 131.7369\n",
      "Epoch 125/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 130.7290 - val_loss: 129.7698\n",
      "Epoch 126/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 128.1016 - val_loss: 127.4238\n",
      "Epoch 127/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 126.4879 - val_loss: 125.1708\n",
      "Epoch 128/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 124.1468 - val_loss: 123.5073\n",
      "Epoch 129/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 121.6141 - val_loss: 121.1108\n",
      "Epoch 130/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 120.2408 - val_loss: 119.5053\n",
      "Epoch 131/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 117.6538 - val_loss: 117.1592\n",
      "Epoch 132/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 115.6996 - val_loss: 115.5792\n",
      "Epoch 133/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 113.8432 - val_loss: 113.6547\n",
      "Epoch 134/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 112.7474 - val_loss: 112.2501\n",
      "Epoch 135/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 110.8229 - val_loss: 110.1046\n",
      "Epoch 136/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 108.9121 - val_loss: 109.6438\n",
      "Epoch 137/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 107.8858 - val_loss: 107.3120\n",
      "Epoch 138/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 105.7092 - val_loss: 106.0267\n",
      "Epoch 139/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 103.9228 - val_loss: 103.7736\n",
      "Epoch 140/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 104.1555 - val_loss: 103.4148\n",
      "Epoch 141/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 101.9746 - val_loss: 101.2278\n",
      "Epoch 142/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 99.9389 - val_loss: 99.8252\n",
      "Epoch 143/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 98.5831 - val_loss: 99.0150\n",
      "Epoch 144/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 97.8893 - val_loss: 97.2785\n",
      "Epoch 145/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 96.2364 - val_loss: 96.4118\n",
      "Epoch 146/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 95.7327 - val_loss: 95.2256\n",
      "Epoch 147/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 93.9338 - val_loss: 94.2373\n",
      "Epoch 148/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 92.9955 - val_loss: 92.9242\n",
      "Epoch 149/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 91.8424 - val_loss: 92.5277\n",
      "Epoch 150/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 91.1118 - val_loss: 91.4820\n",
      "Epoch 151/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 89.6785 - val_loss: 90.0102\n",
      "Epoch 152/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 88.9122 - val_loss: 88.7056\n",
      "Epoch 153/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 88.8238 - val_loss: 88.2336\n",
      "Epoch 154/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 87.2254 - val_loss: 88.1501\n",
      "Epoch 155/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 86.3762 - val_loss: 86.2308\n",
      "Epoch 156/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 85.5913 - val_loss: 85.7478\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 84.5568 - val_loss: 84.5030\n",
      "Epoch 158/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 84.0205 - val_loss: 83.5353\n",
      "Epoch 159/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 83.1156 - val_loss: 82.8901\n",
      "Epoch 160/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 82.5327 - val_loss: 82.1641\n",
      "Epoch 161/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 81.6521 - val_loss: 81.7434\n",
      "Epoch 162/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 80.7267 - val_loss: 81.3649\n",
      "Epoch 163/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 80.1931 - val_loss: 80.3336\n",
      "Epoch 164/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 79.5060 - val_loss: 79.5332\n",
      "Epoch 165/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 78.9042 - val_loss: 79.0374\n",
      "Epoch 166/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 78.7389 - val_loss: 78.0653\n",
      "Epoch 167/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 78.0293 - val_loss: 78.9335\n",
      "Epoch 168/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 77.4197 - val_loss: 76.8967\n",
      "Epoch 169/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 76.7559 - val_loss: 76.5005\n",
      "Epoch 170/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 76.2631 - val_loss: 76.5475\n",
      "Epoch 171/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 75.7980 - val_loss: 75.9903\n",
      "Epoch 172/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 75.3357 - val_loss: 75.1485\n",
      "Epoch 173/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 74.7177 - val_loss: 74.6680\n",
      "Epoch 174/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 74.4607 - val_loss: 73.7946\n",
      "Epoch 175/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 74.2269 - val_loss: 73.2958\n",
      "Epoch 176/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 73.9352 - val_loss: 73.0759\n",
      "Epoch 177/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 73.1246 - val_loss: 72.4852\n",
      "Epoch 178/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 72.4042 - val_loss: 72.8648\n",
      "Epoch 179/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 72.7683 - val_loss: 72.1493\n",
      "Epoch 180/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 71.6969 - val_loss: 71.5034\n",
      "Epoch 181/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 71.3911 - val_loss: 71.5350\n",
      "Epoch 182/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 70.9036 - val_loss: 70.9445\n",
      "Epoch 183/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 71.9000 - val_loss: 70.5560\n",
      "Epoch 184/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 70.5153 - val_loss: 69.8252\n",
      "Epoch 185/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 69.9719 - val_loss: 70.1642\n",
      "Epoch 186/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 69.7163 - val_loss: 69.9268\n",
      "Epoch 187/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 69.5831 - val_loss: 68.6412\n",
      "Epoch 188/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 68.8534 - val_loss: 68.3104\n",
      "Epoch 189/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 68.4653 - val_loss: 68.5871\n",
      "Epoch 190/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 68.4825 - val_loss: 67.6890\n",
      "Epoch 191/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 67.8266 - val_loss: 67.5595\n",
      "Epoch 192/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 67.5868 - val_loss: 67.1286\n",
      "Epoch 193/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 67.5940 - val_loss: 66.5573\n",
      "Epoch 194/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 67.3512 - val_loss: 67.3344\n",
      "Epoch 195/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 66.9987 - val_loss: 66.5111\n",
      "Epoch 196/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 67.1474 - val_loss: 66.1151\n",
      "Epoch 197/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 66.2880 - val_loss: 65.7138\n",
      "Epoch 198/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 66.0548 - val_loss: 65.7897\n",
      "Epoch 199/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 65.9835 - val_loss: 65.1758\n",
      "Epoch 200/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 65.2864 - val_loss: 64.7674\n",
      "Epoch 201/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 65.3991 - val_loss: 64.5644\n",
      "Epoch 202/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 65.1086 - val_loss: 64.2494\n",
      "Epoch 203/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 64.7608 - val_loss: 64.1697\n",
      "Epoch 204/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 64.6626 - val_loss: 63.5103\n",
      "Epoch 205/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 65.1462 - val_loss: 63.2183\n",
      "Epoch 206/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 64.3491 - val_loss: 63.5673\n",
      "Epoch 207/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 63.7590 - val_loss: 62.7702\n",
      "Epoch 208/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 63.9968 - val_loss: 64.4367\n",
      "Epoch 209/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 63.4351 - val_loss: 62.4162\n",
      "Epoch 210/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 63.2925 - val_loss: 61.9311\n",
      "Epoch 211/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 63.0497 - val_loss: 61.7619\n",
      "Epoch 212/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 63.7006 - val_loss: 61.4612\n",
      "Epoch 213/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 62.6663 - val_loss: 61.3594\n",
      "Epoch 214/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 61.9555 - val_loss: 61.5497\n",
      "Epoch 215/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 61.6748 - val_loss: 60.9821\n",
      "Epoch 216/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 61.5751 - val_loss: 61.3923\n",
      "Epoch 217/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 61.4247 - val_loss: 61.3416\n",
      "Epoch 218/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 61.3046 - val_loss: 60.0831\n",
      "Epoch 219/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 60.7594 - val_loss: 60.3468\n",
      "Epoch 220/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 60.5812 - val_loss: 60.0518\n",
      "Epoch 221/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 60.5177 - val_loss: 59.4150\n",
      "Epoch 222/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 60.0874 - val_loss: 59.4766\n",
      "Epoch 223/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 60.0029 - val_loss: 58.7697\n",
      "Epoch 224/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 60.1670 - val_loss: 58.8314\n",
      "Epoch 225/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 59.5597 - val_loss: 58.9289\n",
      "Epoch 226/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 59.8744 - val_loss: 59.8223\n",
      "Epoch 227/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 59.6709 - val_loss: 57.9972\n",
      "Epoch 228/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 58.8289 - val_loss: 57.7514\n",
      "Epoch 229/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 59.0336 - val_loss: 58.7366\n",
      "Epoch 230/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 58.4708 - val_loss: 57.4276\n",
      "Epoch 231/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 58.9480 - val_loss: 58.4418\n",
      "Epoch 232/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 57.9931 - val_loss: 56.5173\n",
      "Epoch 233/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 58.2872 - val_loss: 57.2654\n",
      "Epoch 234/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 57.2622 - val_loss: 56.1437\n",
      "Epoch 235/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 57.0856 - val_loss: 56.2341\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 56.8026 - val_loss: 55.6217\n",
      "Epoch 237/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 56.5148 - val_loss: 55.7112\n",
      "Epoch 238/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 56.4034 - val_loss: 56.1439\n",
      "Epoch 239/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 56.3711 - val_loss: 54.7582\n",
      "Epoch 240/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 55.9269 - val_loss: 54.7453\n",
      "Epoch 241/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 55.4758 - val_loss: 54.9093\n",
      "Epoch 242/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 55.1648 - val_loss: 54.0676\n",
      "Epoch 243/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 55.0182 - val_loss: 53.8066\n",
      "Epoch 244/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 54.7450 - val_loss: 53.7124\n",
      "Epoch 245/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 54.5875 - val_loss: 54.2415\n",
      "Epoch 246/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 54.3880 - val_loss: 53.0843\n",
      "Epoch 247/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 53.9058 - val_loss: 53.1078\n",
      "Epoch 248/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 53.7669 - val_loss: 52.7572\n",
      "Epoch 249/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 53.4279 - val_loss: 52.3675\n",
      "Epoch 250/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 53.1506 - val_loss: 52.8558\n",
      "Epoch 251/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 52.8399 - val_loss: 52.0238\n",
      "Epoch 252/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 52.5809 - val_loss: 51.6779\n",
      "Epoch 253/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 52.2986 - val_loss: 51.9499\n",
      "Epoch 254/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 52.8585 - val_loss: 51.0325\n",
      "Epoch 255/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 52.0659 - val_loss: 50.6301\n",
      "Epoch 256/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 51.9594 - val_loss: 51.9135\n",
      "Epoch 257/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 51.5371 - val_loss: 50.1822\n",
      "Epoch 258/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 51.0487 - val_loss: 49.9850\n",
      "Epoch 259/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 50.8244 - val_loss: 50.0273\n",
      "Epoch 260/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 50.6570 - val_loss: 50.5430\n",
      "Epoch 261/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 50.2667 - val_loss: 49.4921\n",
      "Epoch 262/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 50.1637 - val_loss: 49.4965\n",
      "Epoch 263/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 49.7122 - val_loss: 48.6617\n",
      "Epoch 264/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 49.4409 - val_loss: 48.7556\n",
      "Epoch 265/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 49.0174 - val_loss: 48.2153\n",
      "Epoch 266/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 48.7216 - val_loss: 47.6728\n",
      "Epoch 267/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 48.7534 - val_loss: 47.3815\n",
      "Epoch 268/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 48.5931 - val_loss: 47.6743\n",
      "Epoch 269/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 48.2559 - val_loss: 47.2846\n",
      "Epoch 270/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 47.6260 - val_loss: 46.7099\n",
      "Epoch 271/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 47.6693 - val_loss: 46.2321\n",
      "Epoch 272/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 47.2507 - val_loss: 47.1495\n",
      "Epoch 273/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 47.2201 - val_loss: 46.3688\n",
      "Epoch 274/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 46.6447 - val_loss: 45.8288\n",
      "Epoch 275/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 46.3789 - val_loss: 45.2403\n",
      "Epoch 276/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 46.2003 - val_loss: 45.2626\n",
      "Epoch 277/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 46.0062 - val_loss: 45.5428\n",
      "Epoch 278/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 45.6640 - val_loss: 44.3778\n",
      "Epoch 279/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 45.3851 - val_loss: 44.6035\n",
      "Epoch 280/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 45.5588 - val_loss: 44.8560\n",
      "Epoch 281/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 44.7659 - val_loss: 43.9266\n",
      "Epoch 282/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 44.5364 - val_loss: 43.6384\n",
      "Epoch 283/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 44.5603 - val_loss: 43.2525\n",
      "Epoch 284/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 44.1666 - val_loss: 43.2425\n",
      "Epoch 285/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 43.7873 - val_loss: 42.8228\n",
      "Epoch 286/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 43.8789 - val_loss: 42.3147\n",
      "Epoch 287/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 44.1012 - val_loss: 43.4475\n",
      "Epoch 288/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 43.2059 - val_loss: 41.8917\n",
      "Epoch 289/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 42.9410 - val_loss: 41.9812\n",
      "Epoch 290/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 42.6761 - val_loss: 41.5258\n",
      "Epoch 291/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 42.3474 - val_loss: 41.2691\n",
      "Epoch 292/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 42.0140 - val_loss: 40.9863\n",
      "Epoch 293/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 41.6857 - val_loss: 41.1004\n",
      "Epoch 294/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 41.5821 - val_loss: 40.6516\n",
      "Epoch 295/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 41.4139 - val_loss: 40.0663\n",
      "Epoch 296/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 41.2236 - val_loss: 39.9219\n",
      "Epoch 297/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 41.0077 - val_loss: 39.6023\n",
      "Epoch 298/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 40.6211 - val_loss: 40.5559\n",
      "Epoch 299/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 40.6472 - val_loss: 39.2375\n",
      "Epoch 300/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 40.0938 - val_loss: 40.1696\n",
      "Epoch 301/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 39.9905 - val_loss: 39.1231\n",
      "Epoch 302/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 39.6635 - val_loss: 38.4006\n",
      "Epoch 303/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 39.3034 - val_loss: 38.4917\n",
      "Epoch 304/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 39.0411 - val_loss: 38.1292\n",
      "Epoch 305/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 39.0172 - val_loss: 37.9522\n",
      "Epoch 306/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 38.5077 - val_loss: 37.8922\n",
      "Epoch 307/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 38.1012 - val_loss: 37.1643\n",
      "Epoch 308/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 38.0396 - val_loss: 37.6434\n",
      "Epoch 309/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 38.1612 - val_loss: 36.6620\n",
      "Epoch 310/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 38.1025 - val_loss: 36.4478\n",
      "Epoch 311/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 37.3599 - val_loss: 36.5100\n",
      "Epoch 312/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 36.9632 - val_loss: 36.1551\n",
      "Epoch 313/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 37.1779 - val_loss: 36.1095\n",
      "Epoch 314/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 36.6414 - val_loss: 35.6347\n",
      "Epoch 315/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 36.3081 - val_loss: 35.6610\n",
      "Epoch 316/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 36.3670 - val_loss: 35.0366\n",
      "Epoch 317/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 35.9049 - val_loss: 35.5054\n",
      "Epoch 318/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 35.8178 - val_loss: 35.0774\n",
      "Epoch 319/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 35.6963 - val_loss: 34.4127\n",
      "Epoch 320/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 35.1369 - val_loss: 34.8125\n",
      "Epoch 321/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 35.1095 - val_loss: 34.0907\n",
      "Epoch 322/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 34.7961 - val_loss: 34.5952\n",
      "Epoch 323/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 34.6133 - val_loss: 34.0172\n",
      "Epoch 324/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 34.3217 - val_loss: 33.6481\n",
      "Epoch 325/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 34.0667 - val_loss: 33.2911\n",
      "Epoch 326/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 34.0969 - val_loss: 33.2297\n",
      "Epoch 327/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 33.8472 - val_loss: 33.5536\n",
      "Epoch 328/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 33.5363 - val_loss: 32.6805\n",
      "Epoch 329/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 33.1349 - val_loss: 32.8382\n",
      "Epoch 330/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 33.4260 - val_loss: 32.4926\n",
      "Epoch 331/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 32.8389 - val_loss: 32.7519\n",
      "Epoch 332/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 32.6965 - val_loss: 31.9022\n",
      "Epoch 333/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 32.3224 - val_loss: 31.7026\n",
      "Epoch 334/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 32.1350 - val_loss: 31.4889\n",
      "Epoch 335/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 32.0482 - val_loss: 31.4004\n",
      "Epoch 336/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 31.9235 - val_loss: 32.4534\n",
      "Epoch 337/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 31.7682 - val_loss: 30.8360\n",
      "Epoch 338/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 31.7426 - val_loss: 30.6051\n",
      "Epoch 339/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 31.7100 - val_loss: 30.4533\n",
      "Epoch 340/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 31.1565 - val_loss: 30.5996\n",
      "Epoch 341/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 30.9077 - val_loss: 30.7899\n",
      "Epoch 342/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 31.0351 - val_loss: 29.9710\n",
      "Epoch 343/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 30.3839 - val_loss: 29.9443\n",
      "Epoch 344/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 30.3005 - val_loss: 29.8959\n",
      "Epoch 345/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 30.0827 - val_loss: 29.5187\n",
      "Epoch 346/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 29.9422 - val_loss: 30.0656\n",
      "Epoch 347/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 29.7367 - val_loss: 29.4213\n",
      "Epoch 348/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 29.9199 - val_loss: 29.1761\n",
      "Epoch 349/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 29.5248 - val_loss: 29.4201\n",
      "Epoch 350/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 29.2777 - val_loss: 28.7859\n",
      "Epoch 351/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 28.9820 - val_loss: 29.0812\n",
      "Epoch 352/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 28.9621 - val_loss: 28.2806\n",
      "Epoch 353/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 28.6276 - val_loss: 28.5189\n",
      "Epoch 354/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 28.5083 - val_loss: 28.2496\n",
      "Epoch 355/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 28.3530 - val_loss: 27.9839\n",
      "Epoch 356/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 28.2350 - val_loss: 28.2391\n",
      "Epoch 357/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 28.1666 - val_loss: 27.8174\n",
      "Epoch 358/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 28.4027 - val_loss: 27.3868\n",
      "Epoch 359/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 27.8413 - val_loss: 27.3418\n",
      "Epoch 360/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 27.5932 - val_loss: 27.5228\n",
      "Epoch 361/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 27.4396 - val_loss: 27.1817\n",
      "Epoch 362/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 27.3768 - val_loss: 26.8040\n",
      "Epoch 363/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 27.3583 - val_loss: 26.6815\n",
      "Epoch 364/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 27.0437 - val_loss: 26.7747\n",
      "Epoch 365/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 26.8567 - val_loss: 26.6425\n",
      "Epoch 366/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 26.8647 - val_loss: 26.5002\n",
      "Epoch 367/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 26.8283 - val_loss: 26.0115\n",
      "Epoch 368/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 26.8212 - val_loss: 25.9639\n",
      "Epoch 369/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 26.5630 - val_loss: 26.0101\n",
      "Epoch 370/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 26.2425 - val_loss: 26.4460\n",
      "Epoch 371/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 26.0759 - val_loss: 25.6069\n",
      "Epoch 372/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 25.9977 - val_loss: 25.7508\n",
      "Epoch 373/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 26.0394 - val_loss: 25.5708\n",
      "Epoch 374/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 25.7351 - val_loss: 25.3911\n",
      "Epoch 375/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 25.7391 - val_loss: 25.0746\n",
      "Epoch 376/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 25.5656 - val_loss: 25.5931\n",
      "Epoch 377/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 25.4035 - val_loss: 24.9251\n",
      "Epoch 378/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 25.1221 - val_loss: 25.0642\n",
      "Epoch 379/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 25.0469 - val_loss: 24.8275\n",
      "Epoch 380/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 25.1907 - val_loss: 24.4626\n",
      "Epoch 381/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 24.9533 - val_loss: 24.9745\n",
      "Epoch 382/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.8481 - val_loss: 25.0113\n",
      "Epoch 383/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.8877 - val_loss: 24.2286\n",
      "Epoch 384/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.9595 - val_loss: 25.5433\n",
      "Epoch 385/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 24.5922 - val_loss: 23.9593\n",
      "Epoch 386/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.3458 - val_loss: 24.4461\n",
      "Epoch 387/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.2375 - val_loss: 24.0842\n",
      "Epoch 388/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.1955 - val_loss: 24.3703\n",
      "Epoch 389/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 23.9720 - val_loss: 23.7250\n",
      "Epoch 390/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.2964 - val_loss: 24.5464\n",
      "Epoch 391/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 24.0063 - val_loss: 23.7666\n",
      "Epoch 392/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 23.8504 - val_loss: 23.4912\n",
      "Epoch 393/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 23.9062 - val_loss: 23.5376\n",
      "Epoch 394/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 5ms/step - loss: 23.7032 - val_loss: 23.4793\n",
      "Epoch 395/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 23.4643 - val_loss: 23.7321\n",
      "Epoch 396/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 23.8172 - val_loss: 23.0342\n",
      "Epoch 397/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 23.7348 - val_loss: 23.2355\n",
      "Epoch 398/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 23.5208 - val_loss: 23.2509\n",
      "Epoch 399/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 23.2611 - val_loss: 23.0554\n",
      "Epoch 400/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 23.1047 - val_loss: 23.1126\n",
      "Epoch 401/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.9777 - val_loss: 22.8750\n",
      "Epoch 402/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.9357 - val_loss: 23.0630\n",
      "Epoch 403/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.8701 - val_loss: 22.7408\n",
      "Epoch 404/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.9267 - val_loss: 23.1505\n",
      "Epoch 405/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.9233 - val_loss: 23.2828\n",
      "Epoch 406/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.7801 - val_loss: 22.3805\n",
      "Epoch 407/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.7263 - val_loss: 22.4915\n",
      "Epoch 408/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.6279 - val_loss: 22.3473\n",
      "Epoch 409/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.4641 - val_loss: 22.3801\n",
      "Epoch 410/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.3628 - val_loss: 22.2337\n",
      "Epoch 411/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 22.3562 - val_loss: 22.4669\n",
      "Epoch 412/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.4044 - val_loss: 22.4523\n",
      "Epoch 413/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.3071 - val_loss: 22.3873\n",
      "Epoch 414/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.5494 - val_loss: 22.1668\n",
      "Epoch 415/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.0872 - val_loss: 22.2208\n",
      "Epoch 416/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.0085 - val_loss: 21.8748\n",
      "Epoch 417/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 22.0447 - val_loss: 21.9145\n",
      "Epoch 418/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.8470 - val_loss: 22.0528\n",
      "Epoch 419/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.7825 - val_loss: 21.9198\n",
      "Epoch 420/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 21.9469 - val_loss: 21.7772\n",
      "Epoch 421/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.7290 - val_loss: 21.8541\n",
      "Epoch 422/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.6302 - val_loss: 21.7969\n",
      "Epoch 423/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.6571 - val_loss: 22.1933\n",
      "Epoch 424/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 21.5818 - val_loss: 21.7889\n",
      "Epoch 425/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 21.4034 - val_loss: 21.4634\n",
      "Epoch 426/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 21.4023 - val_loss: 21.7877\n",
      "Epoch 427/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.5084 - val_loss: 21.2821\n",
      "Epoch 428/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.3808 - val_loss: 21.4271\n",
      "Epoch 429/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 21.3566 - val_loss: 21.9232\n",
      "Epoch 430/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 21.3596 - val_loss: 21.5799\n",
      "Epoch 431/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 21.2804 - val_loss: 21.2828\n",
      "Epoch 432/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 21.1580 - val_loss: 21.0554\n",
      "Epoch 433/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 21.1144 - val_loss: 22.1369\n",
      "Epoch 434/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 21.4487 - val_loss: 21.3665\n",
      "Epoch 435/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 21.0807 - val_loss: 21.0177\n",
      "Epoch 436/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.9157 - val_loss: 21.0750\n",
      "Epoch 437/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.8979 - val_loss: 21.0721\n",
      "Epoch 438/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.9935 - val_loss: 20.7949\n",
      "Epoch 439/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 20.9417 - val_loss: 20.9181\n",
      "Epoch 440/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.7627 - val_loss: 20.9640\n",
      "Epoch 441/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.6956 - val_loss: 20.8932\n",
      "Epoch 442/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.9925 - val_loss: 20.6010\n",
      "Epoch 443/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.6251 - val_loss: 20.7908\n",
      "Epoch 444/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 20.7198 - val_loss: 20.7062\n",
      "Epoch 445/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.4825 - val_loss: 21.3029\n",
      "Epoch 446/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.5668 - val_loss: 20.7654\n",
      "Epoch 447/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 20.3927 - val_loss: 20.6011\n",
      "Epoch 448/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.3151 - val_loss: 20.8016\n",
      "Epoch 449/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.5415 - val_loss: 20.7133\n",
      "Epoch 450/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.2677 - val_loss: 20.4246\n",
      "Epoch 451/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 20.3259 - val_loss: 20.4232\n",
      "Epoch 452/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.2951 - val_loss: 20.7060\n",
      "Epoch 453/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.3347 - val_loss: 20.7311\n",
      "Epoch 454/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.1909 - val_loss: 20.5558\n",
      "Epoch 455/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.2241 - val_loss: 21.2835\n",
      "Epoch 456/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.0750 - val_loss: 20.2615\n",
      "Epoch 457/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.1102 - val_loss: 20.3718\n",
      "Epoch 458/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.0005 - val_loss: 20.4933\n",
      "Epoch 459/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.0642 - val_loss: 20.4318\n",
      "Epoch 460/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.9568 - val_loss: 20.1749\n",
      "Epoch 461/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.8819 - val_loss: 20.2509\n",
      "Epoch 462/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.8543 - val_loss: 20.3310\n",
      "Epoch 463/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.9902 - val_loss: 20.5224\n",
      "Epoch 464/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 20.0782 - val_loss: 19.8840\n",
      "Epoch 465/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.8204 - val_loss: 20.4565\n",
      "Epoch 466/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.7362 - val_loss: 20.1776\n",
      "Epoch 467/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.7715 - val_loss: 20.3373\n",
      "Epoch 468/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.7723 - val_loss: 20.2423\n",
      "Epoch 469/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.6059 - val_loss: 20.0158\n",
      "Epoch 470/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.5874 - val_loss: 20.0689\n",
      "Epoch 471/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.6909 - val_loss: 20.7619\n",
      "Epoch 472/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.9559 - val_loss: 19.7764\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 19.9197 - val_loss: 19.7051\n",
      "Epoch 474/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.5996 - val_loss: 19.7675\n",
      "Epoch 475/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.4481 - val_loss: 20.1313\n",
      "Epoch 476/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.4204 - val_loss: 20.2228\n",
      "Epoch 477/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.4207 - val_loss: 19.9319\n",
      "Epoch 478/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.6481 - val_loss: 19.5349\n",
      "Epoch 479/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 19.7809 - val_loss: 20.4103\n",
      "Epoch 480/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 19.4279 - val_loss: 20.0077\n",
      "Epoch 481/500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 19.2918 - val_loss: 19.6128\n",
      "Epoch 482/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 19.3056 - val_loss: 19.7728\n",
      "Epoch 483/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.2367 - val_loss: 19.9715\n",
      "Epoch 484/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 19.3989 - val_loss: 19.5273\n",
      "Epoch 485/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 19.1302 - val_loss: 20.0274\n",
      "Epoch 486/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 19.2189 - val_loss: 20.1388\n",
      "Epoch 487/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.3798 - val_loss: 20.4192\n",
      "Epoch 488/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.0618 - val_loss: 19.3785\n",
      "Epoch 489/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.3116 - val_loss: 19.6069\n",
      "Epoch 490/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.2919 - val_loss: 20.4202\n",
      "Epoch 491/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.2153 - val_loss: 19.9429\n",
      "Epoch 492/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.0658 - val_loss: 19.5480\n",
      "Epoch 493/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 18.9414 - val_loss: 19.6760\n",
      "Epoch 494/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.0008 - val_loss: 20.0535\n",
      "Epoch 495/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 19.0084 - val_loss: 19.2752\n",
      "Epoch 496/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 19.0469 - val_loss: 19.2829\n",
      "Epoch 497/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 18.9455 - val_loss: 20.1103\n",
      "Epoch 498/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 18.8136 - val_loss: 19.2065\n",
      "Epoch 499/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 19.0171 - val_loss: 19.8531\n",
      "Epoch 500/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 18.8796 - val_loss: 19.2285\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history=model.fit(X_train, y_train, validation_data =(X_test, y_test),epochs=500, batch_size=10) #verbose=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18.730, Test: 19.229\n"
     ]
    }
   ],
   "source": [
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves of mean squared error  on the train and test set at the end of each training epoch are graphed below using line plots. These learning curves give an indication of the dynamics while learning the model.\n",
    "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "plt.title('Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "\n",
    "The model can be updated to scale the target variable. \n",
    ">Reducing the scale of the target variable will, in turn, reduce the size of the gradient used to update the weights and result in a more stable model and training process.\n",
    "\n",
    "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "\n",
    "According to this article, a good rule of thumb is that input variables should be small values, probably in the range of 0-1 or standardized with a zero mean and a standard deviation of one.\n",
    ">If the distribution of the quantity is normal, then it should be standardized, otherwise the data should be normalized.\n",
    "If in doubt, normalise the input sequence.\n",
    "\n",
    ">The output variable is the variable predicted by the network. If the distribution of the value is normal, then you can standardize the output variable. Otherwise, the output variable can be normalized.\n",
    "\n",
    "The distribution of the speed variable in the dataset is not normal so it should be normalised rather than standardised. \n",
    "Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "\n",
    "The article indicates that good practice with the `MinMaxScaler` and other scaling techniques is to :\n",
    "- Fit the scaler using the available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. Use `fit()` to do this.\n",
    "- Apply the scale to the training data. Use the normalised data to train the model. Use `transform()` to do this.\n",
    "- Apply the scale going forward.\n",
    "`fit_transform()` function can do the two steps in one go.\n",
    "\n",
    "`scaler = MinMaxScaler()` \n",
    "`normalized = scaler.fit_transform(data)` \n",
    "\n",
    "Scikit-learn's preprocessing module can be used to scale the data.\n",
    ">Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
    "\n",
    "The data can be scaled to lie between a given minimum and maximum value such as between zero and one or so that the maximum absolute value of each feature is scaled to unit size. \n",
    "\n",
    "`scaler = MinMaxScaler(feature_range=(-1,1))`\n",
    "\n",
    "\n",
    "The [Introduction to Keras for Engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/) does suggest that data preprocessing should be done such as feature normalisation.\n",
    "- Do preprocessing such as feature normalisation.\n",
    "The data consists of two columns of data and thats it. The only preprocesing that is applicable here is to rescale the data to small values as in general the input values to a neural network should be close to zero. According to the tutorial  the data should either be rescaled to have zero-mean and unit-variance or the data in the `[0.1]` range. The preprocessing should ideally be done as part of the model to make it more portable in production. In Keras the preprocessing is done via preprocessing layers which can be included directly into your model either during training or after training. Some preprocessing layers have a state, in this case normalization holds the mean and variance of the features and can be obtained by calling `layer.adapt(data)` on a sample or all of the training data.\n",
    "\n",
    "CSV data needs to be parsed, with numerical features converted to floating point tensors and categorical features indexed and converted to integer tensors. Then each feature typically needs to be normalized to zero-mean and unit-variance.\n",
    "\n",
    "---\n",
    "https://machinelearningmastery.com/data-leakage-machine-learning/\n",
    "https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split#:~:text=3%20Answers&text=Normalization%20across%20instances%20should%20be,data%20from%20the%20training%20set.&text=Using%20any%20information%20coming%20from,the%20evaluation%20of%20the%20performance.\n",
    "\n",
    "Normalization should be done after splitting the data into train and test/validation to avoid data leakage. Data leakage is where information from outside the dataset is used to train the model.\n",
    "Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set. This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.\n",
    "\n",
    "As yet I am not fully sure whether I should scale outputs as well as inputs but I think I should as we only have a single input feature to train the model on and predict the outputs.\n",
    "\n",
    "Continuing with following this blog:\n",
    "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "\n",
    ">It is best practice is to estimate the mean and standard deviation of the training dataset and use these variables to scale the train and test dataset. This is to avoid any data leakage during the model evaluation process.\n",
    ">The scikit-learn transformers expect input data to be matrices of rows and columns, therefore the 1D arrays for the target variable will have to be reshaped into 2D arrays prior to the transforms.\n",
    "\n",
    "I have the data already in 2-d from earlier. Otherwise could do the following:\n",
    "`y_train= y_train.reshape(len(y_train),1)`\n",
    "`y_test = Y_test.reshape(len(y_train), 1)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data\n",
    "- using min_max scaler:\n",
    "Scale the training data first:\n",
    "\n",
    "Tensorflow.keras has methods for scaling the data also but I've seen more online blogs such as machine learning mastery use sklearn to preprocess and scale, then keras-tensorflow to train the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using sklearn\n",
    "Here I will scale both the input and output features using the MinMaxScaler() function. This should be done after the data has been split into train test datasets.\n",
    "- Fit the scale using the available training data using fit()\n",
    "- Apply the scale to the training data using transform()\n",
    "- Apply the scale to data going forward - any new data on which you want to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "# create a scaler:\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# fit scaler on data\n",
    "y_train =min_max_scaler.fit(y_train)\n",
    "# transform training dataset\n",
    "y_test = min_max_scaler.fit(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the input variable"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# fit scaler on data\n",
    "X_train =min_max_scaler.fit(X_train)\n",
    "# transform training dataset\n",
    "X_test = min_max_scaler.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going ahead again without scaling the data:\n",
    "I did add in the BatchNormalization layer though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(50, input_shape=(1,), activation='sigmoid', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "model.add(kr.layers.BatchNormalization())\n",
    "model.add(kr.layers.Dense(1, activation='linear', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "model.compile(kr.optimizers.Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history=model.fit(X_train, y_train, validation_data =(X_test, y_test),epochs=500, batch_size=10) #verbose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=10) #verbose=0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# plot loss during training\n",
    "plt.title('Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note!\n",
    "While the scaling worked the first time, it threw an error the second time even though I started again.\n",
    "Will leave it for now and focus on the web app\n",
    "I am commenting out the cells here. Will try later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_test_minmax = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "training_data = X_train\n",
    "\n",
    "normalizer = Normalization(axis=1)\n",
    "normalizer.adapt(X)\n",
    "\n",
    "\n",
    "normalized_data = normalizer(training_data)\n",
    "print(\"var: %.4f\" % np.var(normalized_data))\n",
    "print(\"mean: %.4f\" % np.mean(normalized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised data using tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "#normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_minmax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot style.\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Plot size.\n",
    "plt.rcParams['figure.figsize'] = [14, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n",
    "\n",
    "Back to the https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/ tutorial. The next section looks at plotting model learning curves which plot the performance of the neural network model over time. It helps determine if the model is learning well and whether it is underfitting or overfitting the training set.To do this you need to update the call to to the `fit` function to include a reference to a validation dataset which is a portion of the training dataset not used to fit the model but instead used to evaluate the performance of the model during training. The fit function will then return a history object containing a trace of performance metrics recorded at the end of each training epoch. A learning curve is a plot of the loss on the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history = model.fit(X_train, y_train, epochs=500, batch_size=10, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.plot(history.history['loss'], 'r.',label='train')\n",
    "plt.plot(history.history['val_loss'], 'b.',label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately the difference is obvious in the learning curve when all the 49 points with zero power values are dropped. It might not be right though!\n",
    "Compromise here with some values dropped where speed is greater than a particular value. I chose 7 for now."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=10) #verbose=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the initial plot I did before comparing the actual data set to the predicted values using the model. When I did this first I didn't split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well the model does\n",
    "plt.plot(dfx['speed'], dfx['power'], 'k.',label='actual')\n",
    "plt.plot(dfx['speed'], model.predict(dfx['speed']), 'r.',label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual data \n",
    "plt.plot(dfx['speed'], dfx['power'], 'k.',label='actual')\n",
    "# plot predicted values for the test set\n",
    "plt.plot(X_test, y_test, 'r.', label='predicted on test set')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "row = [5,10,15,20]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the model\n",
    "Instead of having to retrain the model each time,  it can be saved to a file using the `save()` function on the model and loaded later to make predictions using the `load_model()` function. The model is saved in H5 format, an efficient array storage format.\n",
    "For this I need  the `h5py` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 99.327\n"
     ]
    }
   ],
   "source": [
    "speed = [20.647]\n",
    "yhat= model.predict(speed)\n",
    "print('Predicted: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 100.749\n"
     ]
    }
   ],
   "source": [
    "speed = [22]\n",
    "yhat= model.predict(speed)\n",
    "print('Predicted: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 2.598\n"
     ]
    }
   ],
   "source": [
    "speed = [5]\n",
    "yhat= model.predict(speed)\n",
    "print('Predicted: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update:\n",
    "I ran through this again without scaling either the input or output variables as I ran into problems doing both in the same notebook. Will come back to this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Run each x value through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67.90072  ],\n",
       "       [  2.660132 ],\n",
       "       [  2.967722 ],\n",
       "       [  7.321494 ],\n",
       "       [ 98.781075 ],\n",
       "       [ 45.345455 ],\n",
       "       [ 61.005867 ],\n",
       "       [100.80556  ],\n",
       "       [  2.611556 ],\n",
       "       [ 88.60112  ],\n",
       "       [  2.629535 ],\n",
       "       [  2.927019 ],\n",
       "       [101.69494  ],\n",
       "       [  3.062473 ],\n",
       "       [  2.7557697],\n",
       "       [  6.1082687],\n",
       "       [ 94.48705  ],\n",
       "       [ 28.219444 ],\n",
       "       [100.42241  ],\n",
       "       [ 35.495018 ],\n",
       "       [ 77.74266  ],\n",
       "       [100.863205 ],\n",
       "       [  3.2935352],\n",
       "       [ 11.807308 ],\n",
       "       [  5.880472 ],\n",
       "       [ 72.97981  ],\n",
       "       [ 81.39311  ],\n",
       "       [ 15.781331 ],\n",
       "       [  2.432674 ],\n",
       "       [ 91.28161  ],\n",
       "       [ 43.57943  ],\n",
       "       [ 27.201675 ],\n",
       "       [  3.561316 ],\n",
       "       [ 95.65593  ],\n",
       "       [ 88.979706 ],\n",
       "       [ 22.84214  ],\n",
       "       [  2.3331437],\n",
       "       [101.80902  ],\n",
       "       [  2.7468724],\n",
       "       [ 89.34931  ],\n",
       "       [ 18.497374 ],\n",
       "       [101.626976 ],\n",
       "       [  3.0093098],\n",
       "       [ 65.584625 ],\n",
       "       [101.66122  ],\n",
       "       [ 99.35875  ],\n",
       "       [  2.386005 ],\n",
       "       [  2.9780378],\n",
       "       [ 99.61559  ],\n",
       "       [ 96.241394 ],\n",
       "       [  2.848403 ],\n",
       "       [ 91.495735 ],\n",
       "       [ 79.91272  ],\n",
       "       [  3.806658 ],\n",
       "       [ 55.333904 ],\n",
       "       [101.92915  ],\n",
       "       [  4.5131316],\n",
       "       [101.88472  ],\n",
       "       [  3.2182298],\n",
       "       [ 97.58785  ],\n",
       "       [ 24.487007 ],\n",
       "       [  2.3257756],\n",
       "       [ 72.28597  ],\n",
       "       [101.53063  ],\n",
       "       [  2.9169846],\n",
       "       [  2.35913  ],\n",
       "       [ 42.406376 ],\n",
       "       [ 69.39866  ],\n",
       "       [ 96.48658  ],\n",
       "       [ 98.895905 ],\n",
       "       [ 89.103905 ],\n",
       "       [  2.7119784],\n",
       "       [ 40.656044 ],\n",
       "       [  6.9503875],\n",
       "       [100.68513  ],\n",
       "       [  2.7574315],\n",
       "       [101.094666 ],\n",
       "       [  2.7034323],\n",
       "       [  3.3487566],\n",
       "       [ 20.60204  ],\n",
       "       [ 87.54669  ],\n",
       "       [101.8285   ],\n",
       "       [  4.0040708],\n",
       "       [  2.3183727],\n",
       "       [102.003944 ],\n",
       "       [ 65.322395 ],\n",
       "       [ 99.32525  ],\n",
       "       [101.79914  ],\n",
       "       [100.51337  ],\n",
       "       [ 58.76056  ],\n",
       "       [ 58.478214 ],\n",
       "       [101.06091  ],\n",
       "       [101.111305 ],\n",
       "       [ 20.820066 ],\n",
       "       [101.19274  ],\n",
       "       [  3.1394353],\n",
       "       [  3.1653702],\n",
       "       [ 93.49694  ],\n",
       "       [ 57.91164  ],\n",
       "       [  8.789019 ],\n",
       "       [  2.3110263],\n",
       "       [ 95.020805 ],\n",
       "       [101.93779  ],\n",
       "       [100.46834  ],\n",
       "       [ 67.39305  ],\n",
       "       [  4.364229 ],\n",
       "       [ 81.57275  ],\n",
       "       [ 45.64031  ],\n",
       "       [ 23.539555 ],\n",
       "       [  2.4393926],\n",
       "       [ 97.92675  ],\n",
       "       [ 91.702126 ],\n",
       "       [  6.8608418],\n",
       "       [ 77.53879  ],\n",
       "       [  2.4327726],\n",
       "       [ 47.410995 ],\n",
       "       [ 81.92846  ],\n",
       "       [ 65.84591  ],\n",
       "       [ 20.170076 ],\n",
       "       [101.60357  ],\n",
       "       [ 98.58232  ],\n",
       "       [  2.838851 ],\n",
       "       [ 61.281853 ],\n",
       "       [  2.3100169],\n",
       "       [ 23.07334  ],\n",
       "       [ 79.52285  ],\n",
       "       [ 32.18166  ],\n",
       "       [  3.46642  ],\n",
       "       [ 73.20878  ],\n",
       "       [ 11.086624 ],\n",
       "       [ 49.782753 ],\n",
       "       [ 76.07783  ],\n",
       "       [ 90.95999  ],\n",
       "       [  2.8546672],\n",
       "       [  3.0410545],\n",
       "       [  2.8012705],\n",
       "       [ 23.774548 ],\n",
       "       [ 93.58348  ],\n",
       "       [  2.31023  ],\n",
       "       [  2.7425141],\n",
       "       [  2.9884083],\n",
       "       [ 42.992386 ],\n",
       "       [ 48.8866   ],\n",
       "       [ 21.705574 ],\n",
       "       [ 39.496952 ],\n",
       "       [ 90.29226  ],\n",
       "       [ 98.15561  ],\n",
       "       [101.42656  ],\n",
       "       [ 95.92205  ],\n",
       "       [  2.45347  ],\n",
       "       [  2.3123488],\n",
       "       [ 88.728325 ],\n",
       "       [ 51.84033  ],\n",
       "       [ 99.22281  ],\n",
       "       [100.12659  ],\n",
       "       [ 47.115788 ],\n",
       "       [  3.436076 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 18.7302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.7302188873291"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 867us/step - loss: 19.2285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.22854995727539"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare actual data to predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feab018f640>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test, y_test, 'x')\n",
    "plt.plot(X_test, model_predictions, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to under predict the power values for the lower values of speed. This is as I expected earlier given that when wind speeds are too low to generate power, the stored power is used later. Look back at the article that mentioned this above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ demonstrates a fully connected network structure with three layers.\n",
    "\n",
    "Create a Sequential Model and  add layers one at a time until happy with the network architecture. A sequential neural network is built up in sequential layers.\n",
    "\n",
    "The first thing is to ensure the input layer has the right number of input features. You provide the input layer to the model\n",
    "The dataset here has only a single input feature which is wind speed. How to determine the correct number of layers to use? According to the tutorial its a matter of trial and error really. Generally you need a network large enough to capture the structure of the problem.\n",
    "\n",
    "Fully connected layers are defined using the `Dense` class.  Dense refers to the layer being densely connected to the previous layer. Every node in the previous layer is connected to everything in the current layer.\n",
    "\n",
    "\n",
    "The number of neurons or nodes in the layer as the first argument and specify the activation function using the `activation` argument. The demo uses the `rectified linear unit` activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer. As far as I recall Ian recommended using just the Sigmoid activation function. According to the tutorial, Sigmoid and Tanh activation functions were the preferred choice  for all layers before but that these days better performance is achieved using the ReLU activation function. I can try one or the other.  A `Sigmoid` layer is used on the outer layer to ensure the network output is between 0 and 1. (This problem though is not looking to classify the data into one of two classes.).\n",
    "The input shape is 1 as there is only 1 input variable. (input_dim=1 or input_shape=(1,))\n",
    "The shape of the input to the model is defined as an argument on the first hidden layer. The first line of code that adds the first Dense layer does 2 things, defines the input or visible layer and the first hidden layer.\n",
    "\n",
    "#### compile\n",
    "Once the model is defined, it can be compiled or built. We specify the learning rate and the loss function which we want to minimise.\n",
    "\n",
    "When compiling, some additional properties are required when training the network. Training a network means finding the best set of weights to map inputs to outputs in our dataset. The loss function must be specified\n",
    ">We must specify the loss function to use to evaluate a set of weights, the optimizer is used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "\n",
    "The tutorial uses cross entrophy as the loss argument as it is looking at a binary classification problem. We are to stick with mean squared error I think.\n",
    ">The optimizer is defined below as the efficient stochastic gradient descent algorithm “adam“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a neural network with 50 neurons.\n",
    "model = kr.models.Sequential()\n",
    "# the first layer with one input, kernel_initializer='ones' \n",
    "model.add(kr.layers.Dense(1, input_shape=(1,), activation=\"linear\", kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "\n",
    "model.add(kr.layers.Dense(50, activation='sigmoid'))\n",
    "model.add(kr.layers.Dense(1, activation=\"linear\",  kernel_initializer='glorot_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a different model\n",
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(50, input_shape=(1,), activation='sigmoid', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "model.add(kr.layers.Dense(50, input_shape=(1,), activation='sigmoid', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "model.add(kr.layers.Dense(1, activation='linear', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "model.compile(kr.optimizers.Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows 153 total parameters. This is made up of the single input in the input layer, 50 neurons plus 50 bias terms in the hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model using the sigmoid function as the activation function.\n",
    "Using the sigmoid function instead of the linear function which is clearly not a good fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training a different model. This one has a hidden layer with 60 neurons. A different kernel initialiser is used. the batch size is how many x's are sent it at any one time. Instead of sending one x in and adjusting the weights and the biases, send it 10 at a time. This sometimes has a smoothing effect. It still sends each x in 500 times (for 500 epochs) but 10 at a time instead of 1 at a time.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train a different model.\n",
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(60, input_shape=(1,), activation='sigmoid', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "model.add(kr.layers.Dense(1, activation='linear', kernel_initializer=\"glorot_uniform\", bias_initializer=\"glorot_uniform\"))\n",
    "model.compile(kr.optimizers.Adam(lr=0.001), loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Fit the data.\n",
    "model.fit(df['speed'], df['power'], epochs=500, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero values for power at high levels of speed does seem to be pulling the curve down so maybe I need to take them out. The rest of the data seems to be quite well modelled.\n",
    "I need to read more into this. Overall it is still impressive that with only a single numerical input for speed you can still get such a close match to what the power values look like.\n",
    "\n",
    "The loss is still quite high and did not come down much despite playing around with the parameters.\n",
    "The ideal loss is is zero and the ideal accuracy is 1.0 or 100%.\n",
    ">The goal is to choose a model configuration and training configuration that achieve the lowest loss and highest accuracy possible for a given dataset\n",
    "- https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "I need to split the data also into a training and test set.\n",
    "See https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "`from sklearn.model_selection import train_test_split` \n",
    "`X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.4, random_state=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "Here I am reading through the article referenced, making notes and some paraphrasing. I will reference properly later.\n",
    "\n",
    "An artificial neuron calculates a weighted sum of its input, adds a bias and then decides whether it should be fired or not. $ Y = \\sum{(\\text{weight} * \\text{input})} + \\text{bias}$\n",
    "The value of Y can be anything from -infinity to + infinity. The neuron does not really know the bounds of the value. So how do we decide whether the neuron should fire or not? This is where the activation functions come in. The activation function will check the Y value produced by the neuron and decide whether outside connections should consider this neuron as activated (fired) or not.\n",
    "\n",
    "Consider a threshold based activation function. If the value of Y is above a certain value then it could declare it as activated, otherwise not. In this case the output is 1 (activated) when the value is greater than than the threshold and 0 otherwise. This is a step function. There are drawbacks to using this method as an activation function for neural networks though when the response is not a binary yes or no.\n",
    "\n",
    "A linear activation function A = cx is a straight line function where activation is proportional to input ( which is the weighted sum from neuron ). In this way it will give a range of activations rather than just binary activations. We can connect some neurons and if more than one fire then we could take the max and decide based on that.\n",
    "\n",
    "The derivative w.r.x is c which means that the gradient has no relationship with X. The descent is going to be on a constant gradient. If there is an error in prediction the changes made by back propagation is constant and not depending on the change in input delta(x).\n",
    "Another problem concerns the connected layers where each layer is activated by a linear function. \n",
    "That activation in turn goes into the next level as input and the second layer calculates weighted sum on that input and it in turn, fires based on another linear activation function.\n",
    "No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer. Therefore two or more layers can be replaced by a single layer. The whole network then is equivalent to  a single layer with linear activation.\n",
    "\n",
    "Thw sigmoid function is smooth and looks somewhat like a step function. It is nonlinear in nature and therefore combinations of layers are also non-linear which means that layers can be stacked. It will also give non binary activations unlike the step function. It has a smooth gradient.\n",
    "\n",
    "Between X values -2 to 2, the Y values are very steep.This means that any small changes in the values of X in that region will cause values of Y to change significantly.This means this function has a tendency to bring the Y values to either end of the curve.\n",
    "It tends to bring the activations to either side of the curve making clear distinctions on prediction. Another advantage over linear function is that the output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function and therefore the activations are bound in a range. \n",
    "Sigmoid functions are one of the most widely used activation functions today. The problems with them is that towards either end of the sigmoid function, the Y values tend to respond very less to changes in X. This means that the gradient at that region is going to be small. It gives rise to a problem of “vanishing gradients”. When the activations reach near the “near-horizontal” part of the curve on either sides, the gradient is small or has vanished ( cannot make significant change because of the extremely small value ). The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ). There are ways to work around this problem and sigmoid is still very popular in classification problems.\n",
    "The article also looked at the Tanh activation functions which is a scaled sigmoid function and the ReLu function which gives an output x if x is positive and 0 otherwise. These are both non-linear functions.\n",
    "\n",
    "The article finishes by suggesting how to choose the correct activation function. \n",
    "When you know the function you are trying to approximate has certain characteristics, you can choose an activation function which will approximate the function faster leading to faster training process. For example, a sigmoid works well for a classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.computing.dcu.ie/~humphrys/Notes/Neural/sigmoid.html\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Some resources on wind energy\n",
    "- [The Irish Wind Energy Association (IWEA)](https://www.iwea.com/about-us/about-us)\n",
    "- [ESBN Connected Wind Farms](https://www.esbnetworks.ie/new-connections/generator-connections/generator-connection-statistics). \n",
    "- http://www.eirgridgroup.com/how-the-grid-works/renewables/\n",
    "- [WindEurope](https://windeurope.org/data-and-analysis/)\n",
    "- [how to calculate power output of wind](https://www.windpowerengineering.com/calculate-wind-power-output/)\n",
    "- [calculating energy production from weather forecast in Python](https://medium.com/planet-os/calculating-energy-production-from-weather-forecast-in-python-3c990047daa)\n",
    "- [Wind Turbine Power Curve Modeling Using\n",
    "Advanced Parametric and Nonparametric Methods](http://ieeexplore.ieee.org/iel7/5165391/5433168/06894235.pdf)\n",
    "- https://www.researchgate.net/publication/257748412_Using_machine_learning_to_predict_wind_turbine_power_output\n",
    "\n",
    "- Using machine learning to predict wind turbine power output by A Clifton, L Kilcher, J K Lundquist and P Fleming, ENVIRONMENTAL RESEARCH LETTERS, doi:10.1088/1748-9326/8/2/024009, Environ. Res. Lett. 8 (2013) 024009 (8pp)\n",
    "\n",
    "- Wind Turbine Power Curve Modeling Using Advanced Parametric and Nonparametric Methods by Shahab Shokrzadeh, Student Member, IEEE, Mohammad Jafari Jozani, and Eric Bibeau. IEEE TRANSACTIONS ON SUSTAINABLE ENERGY, VOL. 5, NO. 4, OCTOBER 2014.  Available from https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6894235\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
